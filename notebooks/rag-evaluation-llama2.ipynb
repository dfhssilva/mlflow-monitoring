{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42084110-295b-493a-9b3e-5d8d29ff78b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## LLM RAG Evaluation with MLflow\n",
    "\n",
    "In this notebook, we will demonstrate how to evaluate a RAG system with MLflow. We will use Sonnet 3.5 as the judge model, via a AWS Bedrock API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Requirements\n",
    "\n",
    "Before proceeding with this tutorial, ensure that you install the necessary dependencies using `poetry`\n",
    "\n",
    "```bash\n",
    "    poetry install\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "\n",
    "We need to set up Loka AWS SSO profile `loka-mlengineer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb946228-62fb-4d68-9732-75935c9cb401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/dsilva/.cache/pypoetry/virtualenvs/mlflow-monitoring-pFwyWwNg-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dsilva/.cache/pypoetry/virtualenvs/mlflow-monitoring-pFwyWwNg-py3.12/lib/python3.12/site-packages/ragas/prompt/base.py:9: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.llms.prompt import PromptValue\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.callbacks.manager import get_bedrock_anthropic_callback\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    SemanticSimilarity,\n",
    "    ContextPrecision,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "boto_session = boto3.Session(profile_name=\"loka-mlengineer\", region_name=\"us-east-1\")\n",
    "bedrock_client = boto_session.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "273d1345-95d7-435a-a7b6-a5f3dbb3f073",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a RAG system\n",
    "\n",
    "Use Langchain and FAISS to create a RAG system that answers questions based on the MLflow documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://mlflow.org/docs/latest/index.html', 'title': 'MLflow: A Tool for Managing the Machine Learning Lifecycle', 'language': 'en'}, page_content=\"\\n\\n\\n\\n  \\n\\n\\n\\nMLflow: A Tool for Managing the Machine Learning Lifecycle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.17.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n MLflow\\n\\nMLflow Overview\\nGetting Started with MLflow\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nR API\\nJava API\\nREST API\\nOfficial MLflow Docker Image\\nCommunity Model Flavors\\nTutorials and Examples\\n\\n\\n\\n\\nContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nMLflow: A Tool for Managing the Machine Learning Lifecycle\\n\\n\\n\\n\\n\\n\\nMLflow: A Tool for Managing the Machine Learning Lifecycle \\nMLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in\\nhandling the complexities of the machine learning process. MLflow focuses on the full lifecycle for\\nmachine learning projects, ensuring that each phase is manageable, traceable, and reproducible.\\n\\nMLflow Getting Started Resources \\nIf this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is\\ngetting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to\\nenhance your learning in area-specific guides and tutorials.\\n\\n\\nLearn about MLflowMLflow BasicsMLflow Models IntroductionGenAI QuickstartsDeep Learning Quickstarts\\nLearn about the core components of MLflow\\n\\n\\nQuickstarts\\n\\n                Get Started with MLflow in our 5-minute tutorial\\n\\nGuides\\n\\n                Learn the core components of MLflow with this in-depth guide to Tracking\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn how to perform common tasks in MLflow\\n\\n\\nGuides\\n\\nAutologging tutorial for effortless model tracking\\n            \\n\\nModel Signatures and type validation in MLflow\\n            \\n\\nModel Deployment Quickstart\\n\\n\\nHyperparameter tuning with MLflow\\n            \\n\\n\\n\\n\\n\\n\\n\\nLearn about MLflow Model-related topics\\n\\n\\nGuides\\n\\n                Introduction to Custom Python Models\\n\\n\\nModel dependency management in MLflow\\n            \\n\\nModel Signatures and type validation\\n            \\n\\n\\n\\n\\n\\n\\n\\nGet started with MLflow's GenAI integrations\\n\\n\\nQuickstarts\\n\\nTransformers Text Generation\\n            \\n\\nLangChain Introductory Tutorial\\n            \\n\\nSentence Transformers Basic Embedding Tutorial\\n            \\n\\nOpenAI Quickstart Tutorial\\n            \\n\\n\\n\\n\\n\\n\\n\\nGet started with MLflow's Deep Learning Library integrations\\n\\n\\nQuickstarts\\n\\nTensorFlow\\n\\n\\nPyTorch\\n\\n\\nKeras\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenAI and MLflow \\nExplore the comprehensive GenAI-focused support in MLflow. From MLflow Deployments for GenAI models to the Prompt Engineering UI and native GenAI-focused MLflow flavors like\\nopen-ai, transformers, and sentence-transformers, the tutorials and guides here will help to get you started in leveraging the\\nbenefits of these powerful models, services, and applications.\\nYou’ll learn how MLflow simplifies both using GenAI models and developing solutions that leverage them. Important tasks such as prompt development, evaluation of prompts, comparison of\\nfoundation models, fine-tuning, logging, and deploying production-grade inference servers are all covered by MLflow.\\nExplore the guides and tutorials below to start your journey!\\n\\n\\nGenAI IntegrationsTracingPrompt Engineering UIMLflow AI GatwayGenAI EvaluationRAG\\nExplore the Native MLflow GenAI Integrations\\n\\n\\n\\n\\n\\nTransformers\\n\\n\\n\\n\\n\\nOpenAI\\n\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\n\\nLlamaIndex\\n\\n\\n\\n\\n\\nSentence Transformers\\n\\n\\n\\n\\nLearn about how to instrument your GenAI Workloads with MLflow Tracing\\n\\n\\nGuides\\n\\n\\n                    Learn how to leverage Tracing in MLflow\\n                \\n\\n                    View the Tracing Guide for more information on tracing\\n                \\n\\n                    Learn how to use MLflow autologging with OpenAI for automated trace logging\\n                \\n\\n                    Discover the automated LangChain trace logging with MLflow autologging\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\nExplore the Prompt Engineering UI\\n\\n\\nQuickstarts\\n\\n                Learn how to use the Prompt Engineering UI\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn about managed access to GenAI services with the MLflow AI Gateway\\n\\n\\nGuides\\n\\n                Learn how to use the MLflow AI Gateway\\n\\n\\n                View the in-depth Guide for the MLflow AI Gateway\\n\\n\\n\\n\\n\\n\\n\\n\\nLearn about GenAI Evaluation\\n\\n\\nGuides\\n\\n                Learn how to evaluate your GenAI applications with MLflow\\n            \\n\\n                Discover how to use MLflow Evaluate with the Prompt Engineering UI\\n            \\n\\n\\n\\n\\n\\n\\n\\nLearn about using Retrieval Augmented Generation (RAG) with MLflow\\n\\n\\nGuides\\n\\n                Learn how to work with RAG systems in MLflow\\n            \\n\\n                View the hands-on LangChain RAG Guide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNext \\n\\n\\n\\n\\n          © MLflow Project, a Series of LF Projects, LLC. All rights reserved.\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://mlflow.org/docs/latest/tracking/autolog.html', 'title': 'Automatic Logging with MLflow Tracking', 'language': 'en'}, page_content='\\n\\n\\n\\n  \\n\\n\\n\\nAutomatic Logging with MLflow Tracking\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.17.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n MLflow\\n\\nMLflow Overview\\nGetting Started with MLflow\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nQuickstart\\nConcepts\\nTracking Runs\\nMLflow Tracking APIs\\nAuto Logging\\nManual Logging\\nTracking Tips\\n\\n\\n\\n\\nTracking Datasets\\nExplore Runs and Results\\nSet up the MLflow Tracking Environment\\nFAQ\\n\\n\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nR API\\nJava API\\nREST API\\nOfficial MLflow Docker Image\\nCommunity Model Flavors\\nTutorials and Examples\\n\\n\\n\\n\\nContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nMLflow Tracking \\nMLflow Tracking APIs \\nAutomatic Logging with MLflow Tracking\\n\\n\\n\\n\\n\\n\\nAutomatic Logging with MLflow Tracking \\nAuto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements. All you need to do is to call\\nmlflow.autolog() before your training code.\\nimport mlflow\\n\\nmlflow.autolog()\\n\\nwith mlflow.start_run():\\n    # your training code goes here\\n    ...\\n\\n\\nThis will enable MLflow to automatically log various information about your run, including:\\n\\nMetrics - MLflow pre-selects a set of metrics to log, based on what model and library you use\\nParameters - hyper params specified for the training, plus default values provided by the library if not explicitly set\\nModel Signature - logs Model signature instance, which describes input and output schema of the model\\nArtifacts -  e.g. model checkpoints\\nDataset - dataset object used for training (if applicable), such as tensorflow.data.Dataset\\n\\n\\nHow to Get started \\n\\nStep 1 - Get MLflow \\nMLflow is available on PyPI. If you don’t already have it installed on your system, you can install it with:\\n\\npip install mlflow\\n\\n\\n\\n\\n\\nStep 2 - Insert mlflow.autolog in Your Code \\nFor example, following code snippet shows how to enable autologging for a scikit-learn model:\\nimport mlflow\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\nmlflow.autolog()\\n\\ndb = load_diabetes()\\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\\n\\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\\n# MLflow triggers logging automatically upon model fitting\\nrf.fit(X_train, y_train)\\n\\n\\n\\n\\nStep 3 - Execute Your Code \\n\\npython YOUR_ML_CODE.py\\n\\n\\n\\n\\n\\nStep 4 - View Your Results in the MLflow UI \\nOnce your training job finishes, you can run following command to launch the MLflow UI:\\n\\nmlflow ui --port 8080\\n\\n\\n\\nThen, navigate to http://localhost:8080 in your browser to view the results.\\n\\n\\n\\nCustomize Autologging Behavior \\nYou can also control the behavior of autologging by passing arguments to mlflow.autolog() function.\\nFor example, you can disable logging of model checkpoints and assosiate tags with your run as follows:\\nimport mlflow\\n\\nmlflow.autolog(\\n    log_model_signatures=False,\\n    extra_tags={\"YOUR_TAG\": \"VALUE\"},\\n)\\n\\n\\nSee mlflow.autolog() for the full set of arguments you can use.\\n\\nEnable / Disable Autologging for Specific Libraries \\nOne common use case is to enable/disable autologging for a specific library. For example, if you train your model on PyTorch but use scikit-learn\\nfor data preprocessing, you may want to disable autologging for scikit-learn while keeping it enabled for PyTorch. You can achieve this by either\\n(1) enable autologging only for PyTorch using PyTorch flavor (2) disable autologging for scikit-learn using its flavor with disable=True.\\nimport mlflow\\n\\n# Option 1: Enable autologging only for PyTorch\\nmlflow.pytorch.autolog()\\n\\n# Option 2: Disable autologging for scikit-learn, but enable it for other libraries\\nmlflow.sklearn.autolog(disable=True)\\nmlflow.autolog()\\n\\n\\n\\n\\n\\nSupported Libraries \\n\\nNote\\nThe generic autolog function mlflow.autolog() enables autologging for each supported library you have installed as soon as you import it.\\nAlternatively, you can use library-specific autolog calls such as mlflow.pytorch.autolog() to explicitly enable (or disable) autologging for a particular library.\\n\\nThe following libraries support autologging:\\n\\n\\nFastai\\nGluon\\nKeras/TensorFlow\\nLangChain\\nLlamaIndex\\nLightGBM\\nOpenAI\\nPaddle\\nPySpark\\nPyTorch\\nScikit-learn\\nSpark\\nStatsmodels\\nXGBoost\\n\\n\\nFor flavors that automatically save models as an artifact, additional files for dependency management are logged.\\n\\nFastai \\nCall the generic autolog function mlflow.fastai.autolog() before your training code to enable automatic logging of metrics and parameters.\\nSee an example usage with Fastai.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nfastai\\nuser-specified metrics\\nLogs optimizer data as parameters. For example,\\nepochs, lr, opt_func, etc;\\nLogs the parameters of the EarlyStoppingCallback and\\nOneCycleScheduler callbacks\\n–\\nModel checkpoints are logged to a ‘models’ directory; MLflow Model (fastai Learner model) on training end; Model summary text is logged\\n\\n\\n\\n\\n\\nGluon \\nCall the generic autolog function mlflow.gluon.autolog() before your training code to enable automatic logging of metrics and parameters.\\nSee example usages with Gluon .\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nGluon\\nTraining loss; validation loss; user-specified metrics\\nNumber of layers; optimizer name; learning rate; epsilon\\n–\\nMLflow Model (Gluon model); on training end\\n\\n\\n\\n\\n\\nKeras/TensorFlow \\nCall the generic autolog function or mlflow.tensorflow.autolog() before your training code to enable automatic logging of metrics and parameters. As an example, try running the Keras/Tensorflow example.\\nNote that only versions of tensorflow>=2.3 are supported.\\nThe respective metrics associated with tf.estimator and EarlyStopping are automatically logged.\\nAs an example, try running the Keras/TensorFlow example.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework/module\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\ntf.keras\\nTraining loss; validation loss; user-specified metrics\\nfit() parameters; optimizer name; learning rate; epsilon\\n–\\nModel summary on training start; MLflow Model (Keras model); TensorBoard logs on training end\\n\\ntf.keras.callbacks.EarlyStopping\\nMetrics from the EarlyStopping callbacks. For example,\\nstopped_epoch, restored_epoch,\\nrestore_best_weight, etc\\nfit() parameters from EarlyStopping.\\nFor example, min_delta, patience, baseline,\\nrestore_best_weights, etc\\n–\\n–\\n\\n\\n\\nIf no active run exists when autolog() captures data, MLflow will automatically create a run to log information to.\\nAlso, MLflow will then automatically end the run once training ends via calls to tf.keras.fit().\\nIf a run already exists when autolog() captures data, MLflow will log to that run but not automatically end that run after training. You will have to manually stop the run if you wish to start a new run context for logging to a new run.\\n\\n\\nLangChain \\nCall the generic autolog function mlflow.langchain.autolog() before your training code to enable automatic logging of traces. See LangChain Autologging for more details.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nLangChain\\n–\\n–\\n–\\n\\nTraces\\nMLflow Model (LangChain model) with model signature on training end\\nInput example\\n\\n\\n\\n\\n\\n\\n\\nLlamaIndex \\nCall the generic autolog function mlflow.llama_index.autolog() before your training code to enable automatic logging of traces.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nLlamaIndex\\n–\\n–\\n–\\n\\nTraces\\n\\n\\n\\n\\n\\n\\n\\nLightGBM \\nCall the generic autolog function mlflow.lightgbm.autolog() before your training code to enable automatic logging of metrics and parameters.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nLightGBM\\nuser-specified metrics\\nlightgbm.train parameters\\n–\\nMLflow Model (LightGBM model) with model signature on training end; feature importance; input example\\n\\n\\n\\nIf early stopping is activated, metrics at the best iteration will be logged as an extra step/iteration.\\n\\n\\nOpenAI \\nCall the generic autolog function mlflow.openai.autolog() before your training code to enable automatic logging of artifacts.\\nSee an example usage with OpenAI.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nOpenAI\\n–\\n–\\n–\\n\\nMLflow Model (OpenAI model) with model signature on training end\\nInput example\\n\\n\\n\\n\\n\\n\\n\\nPaddle \\nCall the generic autolog function mlflow.paddle.autolog() before your training code to enable automatic logging of metrics and parameters.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nPaddle\\nuser-specified metrics\\npaddle.Model.fit parameters\\n–\\nMLflow Model (Paddle model) with model signature on training end\\n\\n\\n\\n\\n\\nPySpark \\nCall mlflow.pyspark.ml.autolog() before your training code to enable automatic logging of metrics, params, and models.\\nSee example usage with PySpark.\\nAutologging for pyspark ml estimators captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nPost training metrics obtained by\\nEvaluator.evaluate\\nParameters obtained by\\nEstimator.fit\\n\\nClass name\\nFully qualified class name\\n\\n\\n\\nMLflow Model containing a fitted estimator\\nmetric_info.json for post training metrics\\n\\n\\n\\n\\n\\n\\n\\nPyTorch \\nCall the generic autolog function mlflow.pytorch.autolog() before your PyTorch Lightning training code to enable automatic logging of metrics, parameters, and models. See example usages here. Note\\nthat currently, PyTorch autologging supports only models trained using PyTorch Lightning.\\nAutologging is triggered on calls to pytorch_lightning.trainer.Trainer.fit and captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework/module\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\npytorch_lightning.trainer.Trainer\\nTraining loss; validation loss; average_test_accuracy;\\nuser-defined-metrics.\\nfit() parameters; optimizer name; learning rate; epsilon.\\n–\\nModel summary on training start, MLflow Model (PyTorch model) on training end;\\n\\npytorch_lightning.callbacks.earlystopping\\nTraining loss; validation loss; average_test_accuracy;\\nuser-defined-metrics.\\nMetrics from the EarlyStopping callbacks.\\nFor example, stopped_epoch, restored_epoch,\\nrestore_best_weight, etc.\\nfit() parameters; optimizer name; learning rate; epsilon\\nParameters from the EarlyStopping callbacks.\\nFor example, min_delta, patience, baseline,``restore_best_weights``, etc\\n–\\nModel summary on training start; MLflow Model (PyTorch model) on training end;\\nBest PyTorch model checkpoint, if training stops due to early stopping callback.\\n\\n\\n\\nIf no active run exists when autolog() captures data, MLflow will automatically create a run to log information, ending the run once\\nthe call to pytorch_lightning.trainer.Trainer.fit() completes.\\nIf a run already exists when autolog() captures data, MLflow will log to that run but not automatically end that run after training.\\n\\nNote\\n\\nParameters not explicitly passed by users (parameters that use default values) while using pytorch_lightning.trainer.Trainer.fit() are not currently automatically logged\\nIn case of a multi-optimizer scenario (such as usage of autoencoder), only the parameters for the first optimizer are logged\\n\\n\\n\\n\\nScikit-learn \\nCall mlflow.sklearn.autolog() before your training code to enable automatic logging of sklearn metrics, params, and models.\\nSee example usage here.\\nAutologging for estimators (e.g. LinearRegression) and meta estimators (e.g. Pipeline) creates a single run and logs:\\n\\n\\n\\n\\n\\n\\n\\n\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nTraining score obtained\\nby estimator.score\\nParameters obtained by\\nestimator.get_params\\n\\nClass name\\nFully qualified class name\\n\\n\\nFitted estimator\\n\\n\\n\\nAutologging for parameter search estimators (e.g. GridSearchCV) creates a single parent run and nested child runs\\n- Parent run\\n  - Child run 1\\n  - Child run 2\\n  - ...\\n\\n\\ncontaining the following data:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRun type\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nParent\\nTraining score\\n\\nParameter search estimator’s parameters\\nBest parameter combination\\n\\n\\n\\nClass name\\nFully qualified class name\\n\\n\\n\\nFitted parameter search estimator\\nFitted best estimator\\nSearch results csv file\\n\\n\\n\\nChild\\nCV test score for\\neach parameter combination\\nEach parameter combination\\n\\nClass name\\nFully qualified class name\\n\\n\\n–\\n\\n\\n\\n\\n\\nSpark \\nInitialize a SparkSession with the mlflow-spark JAR attached (e.g.\\nSparkSession.builder.config(\"spark.jars.packages\", \"org.mlflow.mlflow-spark\")) and then\\ncall the generic autolog function mlflow.spark.autolog() to enable automatic logging of Spark datasource\\ninformation at read-time, without the need for explicit\\nlog statements. Note that autologging of Spark ML (MLlib) models is not yet supported.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nSpark\\n–\\n–\\nSingle tag containing source path, version, format. The tag contains one line per datasource\\n–\\n\\n\\n\\n\\nNote\\n\\nMoreover, Spark datasource autologging occurs asynchronously - as such, it’s possible (though unlikely) to see race conditions when launching short-lived MLflow runs that result in datasource information not being logged.\\n\\n\\n\\nImportant\\nWith Pyspark 3.2.0 or above, Spark datasource autologging requires PYSPARK_PIN_THREAD environment variable to be set to false.\\n\\n\\n\\nStatsmodels \\nCall the generic autolog function mlflow.statsmodels.autolog() before your training code to enable automatic logging of metrics and parameters.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nStatsmodels\\nuser-specified metrics\\nstatsmodels.base.model.Model.fit parameters\\n–\\nMLflow Model (statsmodels.base.wrapper.ResultsWrapper) on training end\\n\\n\\n\\n\\nNote\\n\\nEach model subclass that overrides fit expects and logs its own parameters.\\n\\n\\n\\n\\nXGBoost \\nCall the generic autolog function mlflow.xgboost.autolog() before your training code to enable automatic logging of metrics and parameters.\\nAutologging captures the following information:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFramework\\nMetrics\\nParameters\\nTags\\nArtifacts\\n\\nXGBoost\\nuser-specified metrics\\nxgboost.train parameters\\n–\\nMLflow Model (XGBoost model) with model signature on training end; feature importance; input example\\n\\n\\n\\nIf early stopping is activated, metrics at the best iteration will be logged as an extra step/iteration.\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n\\n          © MLflow Project, a Series of LF Projects, LLC. All rights reserved.\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html', 'title': '5 Minute Tracking Server Overview', 'language': 'en'}, page_content='\\n\\n\\n\\n  \\n\\n\\n\\n5 Minute Tracking Server Overview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.17.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n MLflow\\n\\nMLflow Overview\\nGetting Started with MLflow\\nGuidance on Running Tutorials\\nGetting Started Guides\\nMLflow Tracking\\nAutologging Basics\\nRun Comparison Basics\\nTracking Server Quickstart\\n5 Minute Tracking Server Overview\\n\\n\\nModel Registry Quickstart\\n\\n\\nFurther Learning - What’s Next?\\n\\n\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nR API\\nJava API\\nREST API\\nOfficial MLflow Docker Image\\nCommunity Model Flavors\\nTutorials and Examples\\n\\n\\n\\n\\nContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGetting Started with MLflow \\n5 Minute Tracking Server Overview\\n\\n\\n\\n\\n\\n\\n5 Minute Tracking Server Overview \\nIn this guide we will walk you through how to view your MLflow experiment results with different types of\\ntracking server configurations. At a high level, there are 3 ways to view your MLflow experiments:\\n\\n[Method 1] Start your own MLflow server.\\n[Method 2] Use a free hosted tracking server - Databricks Community Edition.\\n[Method 3] Use production Databricks/AzureML.\\n\\nTo choose among these 3 methods, here is our recommendation:\\n\\nIf you have privacy concerns (data/model/tech stack), use Method 1 - start your own server.\\nIf you are a student or an individual researcher, or if you are developing in cloud-based notebooks (e.g., Google\\nColab), use Method 2 - free hosted tracking server.\\nEnterprise users, or if you want to serve or deploy your model for a production use-case, please use\\nMethod 3 - production Databricks/AzureML.\\n\\nOverall Method 2 - free hosted tracking server is the simplest way to get started with MLflow, but please\\npick the method that best suits your needs.\\n\\nMethod 1: Start Your Own MLflow Server \\nDisclaimier: This part of guide is not suitable for running in a cloud-provided IPython environment\\n(e.g., Collab, Databricks). Please follow the guide below in your local machine (laptop/desktop).\\nA hosted tracking server is the simplest way to store and view MLflow experiments, but it is not suitable for\\nevery user. For example, you may not want to expose your data and model to others in your cloud provider account. In this case,\\nyou can use a local hosted MLflow server to store and view your experiments. To do so, there are two steps:\\n\\nStart your MLflow server.\\nConnect MLflow session to the local MLflow server IP by mlflow.set_tracking_uri().\\n\\n\\nStart a Local MLflow Server \\nIf you don’t have MLflow installed, please run the command below to install it:\\n$ pip install mlflow\\n\\n\\nThe installation of MLflow includes the MLflow CLI tool, so you can start a local MLflow server with UI\\nby running the command below in your terminal:\\n$ mlflow ui\\n\\n\\nIt will generate logs with the IP address, for example:\\n(mlflow) [master][~/Documents/mlflow_team/mlflow]$ mlflow ui\\n[2023-10-25 19:39:12 -0700] [50239] [INFO] Starting gunicorn 20.1.0\\n[2023-10-25 19:39:12 -0700] [50239] [INFO] Listening at: http://127.0.0.1:5000 (50239)\\n\\n\\nOpening the URL of the MLflow tracking server in your browser will bring you to the MLflow UI. The image below is from the open source version of the MLflow UI,\\nwhich is a bit different from the MLflow UI on Databricks CE. Below is a screenshot of the landing page:\\n\\n\\nNote\\nIt’s also possible to deploy your own MLflow server on cloud platforms, but it is out of the scope of this guide.\\n\\n\\n\\nConnect MLflow Session to Your Server \\nNow that the server is spun up, let’s connect our MLflow session to the local server. This is very\\nsimilar to how we connect to a remote hosted tracking provider such as the Databricks platform.\\nmlflow.set_tracking_uri(\"http://localhost:5000\")\\n\\n\\nNext, let’s try logging some dummy metrics. We can view these test metrics on the local hosted UI:\\nmlflow.set_experiment(\"check-localhost-connection\")\\n\\nwith mlflow.start_run():\\n    mlflow.log_metric(\"foo\", 1)\\n    mlflow.log_metric(\"bar\", 2)\\n\\n\\nPutting it together you can copy the following code to your editor and save it as log_mlflow_with_localhost.py:\\nimport mlflow\\n\\nmlflow.set_tracking_uri(\"http://localhost:5000\")\\nmlflow.set_experiment(\"check-localhost-connection\")\\n\\nwith mlflow.start_run():\\n    mlflow.log_metric(\"foo\", 1)\\n    mlflow.log_metric(\"bar\", 2)\\n\\n\\nThen execute it by:\\n$ python log_mlflow_with_localhost.py\\n\\n\\n\\n\\nView Experiment on Your MLflow Server \\nNow let’s view your experiment on the local server. Open the URL in your browser, which is http://localhost:5000\\nin our case. In the UI, inside the left sidebar you should see the experiment with name\\n“check-localhost-connection”. Clicking on this experiment name should bring you to the experiment view, similar to what is shown below.\\n\\nClicking on the run (“clumsy-steed-426” in this example, yours will be different) will bring you to the run view, similar as below.\\n\\n\\n\\nConclusion \\nThat’s all about how to start your own MLflow server and view your experiments. Please see the pros and cons\\nof this method below:\\n\\nPros\\n\\nYou have full control of your data and model, which is good for privacy concerns.\\nNo subscription is required.\\nUnlimited quota of experiments/runs.\\nYou can even customize your UI by forking the MLflow repo and modify the UI code.\\n\\n\\nCons\\n\\nRequires manual setup and maintenance.\\nTeam collaboration is harder than using a hosted tracking server.\\nNot suitable for cloud-based notebook, e.g., Google Colab.\\nRequires extra port forwarding if you deploy your server on cloud VM.\\nNo serving support.\\n\\n\\n\\n\\n\\n\\nMethod 2: Use Free Hosted Tracking Server (Databricks Community Edition) \\nNotice: This part of guide can be directly executed in cloud-based notebook, e.g., Google Colab or\\nDatabricks Notebook.\\nDatabricks Community Edition (CE) is the free, limited-use version of the\\ncloud-based big data platform Databricks. Databricks CE users can access a micro-cluster as well as\\na cluster manager and notebook environment. All users can share their notebooks and host them free of\\ncharge with Databricks. You can use Databricks CE to store and view your MLflow experiments without\\nbeing charged.\\nTo use Databricks CE to store and view our MLflow experiments, basically we need to:\\n\\nCreate a free Databricks CE account.\\nSet up Databricks CE authentication in our dev environment.\\nConnect to Databricks CE in our MLflow experiment session.\\n\\nThen the experiment results will be automatically sent to Databricks CE, where you can view it in\\nMLflow experiment UI. Now let’s look at the code.\\n\\nCreate a Databricks CE Account \\nIf you don’t have an account of Databricks CE yet, you can create one\\nhere. The full process should take no longer than 3 minutes.\\n\\n\\nInstall Dependencies \\n%pip install -q mlflow databricks-sdk\\n\\n\\n\\n\\nSet Up Authentication of Databricks CE \\nTo set up Databricks CE authentication, we can use the API mlflow.login(), which will prompt you for required information:\\n\\nDatabricks Host: Use https://community.cloud.databricks.com/\\nUsername: Your email address that signs in Databricks CE.\\nPassword: Your password of Databricks CE.\\n\\nIf the authentication succeeds, you should see a message “Succesfully signed in Databricks!”.\\nimport mlflow\\n\\nmlflow.login()\\n\\n\\n2023/10/25 22:59:27 ERROR mlflow.utils.credentials: Failed to sign in Databricks: default auth: cannot configure default credentials\\nDatabricks Host (should begin with https://): https://community.cloud.databricks.com/\\nUsername: weirdmouse@gmail.com\\nPassword: ··········\\n2023/10/25 22:59:38 INFO mlflow.utils.credentials: Succesfully signed in Databricks!\\n\\n\\n\\n\\nConnect MLflow Session to Databricks CE \\nWe have set up the credentials, now we need to tell MLflow to send the data into Databricks CE.\\nTo do so, we will use mlflow.set_tracking_uri(\"databricks\") to port MLflow to Databricks CE. Basically\\nit is the command below. Please note that you need to always use “databricks” as the keyword.\\nmlflow.set_tracking_uri(\"databricks\")\\n\\n\\nNow you are ready to go! Let’s try starting an MLflow experiment and log some dummy metrics and view it in our UI.\\nmlflow.set_experiment(\"/check-databricks-connection\")\\n\\nwith mlflow.start_run():\\n    mlflow.log_metric(\"foo\", 1)\\n    mlflow.log_metric(\"bar\", 2)\\n\\n\\n2023/10/25 23:15:42 INFO mlflow.tracking.fluent: Experiment with name \\'/check-databricks-ce-connection\\' does not exist. Creating a new experiment.\\n\\n\\n\\n\\nView Your Experiment on Databricks CE \\nNow let’s navigate to Databricks CE to view the experiment result. Log in to your\\nDatabricks CE account, and click on top left to select machine learning\\nin the drop down list. Then click on the experiment icon. See the screenshot below:\\n\\nIn the “Experiments” view, you should be able to find the experiment “/check-databricks-ce-connection”, similar to\\n\\nClicking on the run name, in our example it is “youthful-lamb-287” (it’s a randomly generated name, you will see\\na different name in your CE console), will bring you to the run view, similar to\\n\\nIn the run view, you will see our dummy metrics “foo” and “bar” are logged successfully.\\n\\n\\nConclusion \\nThat’s all about how to use Databricks CE as the tracking server. Please see the pros and cons\\nof this method below:\\n\\nPros\\n\\nEffortless setup.\\nFree.\\nGood for collaboration, e.g., you can share your MLflow experiment with your teammates easily.\\nCompatible for developing on cloud-based notebook, e.g., Google Colab.\\nCompatible for developing on cloud VM.\\n\\n\\nCons\\n\\nHas quota limit of experiments/runs.\\nNo model registration/serving support.\\n\\n\\n\\n\\n\\n\\nMethod 3: Use Production Hosted Tracking Server \\nIf you are an enterprise user and willing to productionize your model, you can use a production platform like\\nDatabricks or Microsoft AzureML. If you use Databricks, MLflow experiment will log your model into the Databricks\\nMLflow server, and you can register your model then serve your model by a few clicks. Serving feature\\nis only available on production Databricks workspace, and not available on Databricks CE.\\nThe method of using production Databricks is the same as using Databricks CE, you only need to\\nchange the host to be the production workspace. For example, https://dbc-1234567-123.cloud.databricks.com.\\nFor more information about how Databricks power your Machine Learning workflow, please refer to the doc\\nhere.\\nTo use AzureML as the tracking server, please read\\nthe doc here\\n\\nConclusion \\nThat’s all about how to use a production platform as the tracking server. Please see the pros and cons\\nof this method below:\\n\\nPros\\n\\nEffortless setup.\\nGood for collaboration, e.g., you can share your MLflow experiment with your teammates easily.\\nCompatible for developing on cloud-based notebook, e.g., Google Colab.\\nCompatible for developing on cloud VM.\\nSeamless model registration/serving support.\\nHigher quota than Databricks CE (pay as you go).\\n\\n\\nCons\\n\\nNot free.\\nNeed to manage a billing account.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n\\n          © MLflow Project, a Series of LF Projects, LLC. All rights reserved.\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://mlflow.org/docs/latest/python_api/mlflow.deployments.html', 'title': 'mlflow.deployments', 'language': 'en'}, page_content='\\n\\n\\n\\n  \\n\\n\\n\\nmlflow.deployments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2.17.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n MLflow\\n\\nMLflow Overview\\nGetting Started with MLflow\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nmlflow\\nMLflow Tracing APIs\\nmlflow.artifacts\\nmlflow.autogen\\nmlflow.catboost\\nmlflow.client\\nmlflow.config\\nmlflow.data\\nmlflow.deployments\\nmlflow.diviner\\nmlflow.entities\\nmlflow.environment_variables\\nmlflow.fastai\\nmlflow.gateway\\nmlflow.gluon\\nmlflow.h2o\\nmlflow.johnsnowlabs\\nmlflow.keras\\nmlflow.langchain\\nmlflow.lightgbm\\nmlflow.llama_index\\nmlflow.metrics\\nmlflow.mleap\\nmlflow.models\\nmlflow.onnx\\nmlflow.paddle\\nmlflow.pmdarima\\nmlflow.projects\\nmlflow.promptflow\\nmlflow.prophet\\nmlflow.pyfunc\\nmlflow.pyspark.ml\\nmlflow.pytorch\\nmlflow.recipes\\nmlflow.sagemaker\\nmlflow.sentence_transformers\\nmlflow.server\\nmlflow.shap\\nmlflow.sklearn\\nmlflow.spacy\\nmlflow.spark\\nmlflow.statsmodels\\nmlflow.system_metrics\\nmlflow.tensorflow\\nmlflow.tracing\\nmlflow.transformers\\nmlflow.types\\nmlflow.utils\\nmlflow.xgboost\\nmlflow.openai\\nLog Levels\\n\\n\\nR API\\nJava API\\nREST API\\nOfficial MLflow Docker Image\\nCommunity Model Flavors\\nTutorials and Examples\\n\\n\\n\\n\\nContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nPython API \\nmlflow.deployments\\n\\n\\n\\n\\n\\n\\nmlflow.deployments \\nExposes functionality for deploying MLflow models to custom serving tools.\\nNote: model deployment to AWS Sagemaker can currently be performed via the\\nmlflow.sagemaker module. Model deployment to Azure can be performed by using the\\nazureml library.\\nMLflow does not currently provide built-in support for any other deployment targets, but support\\nfor custom targets can be installed via third-party plugins. See a list of known plugins\\nhere.\\nThis page largely focuses on the user-facing deployment APIs. For instructions on implementing\\nyour own plugin for deployment to a custom serving tool, see\\nplugin docs.\\n\\n\\nclass mlflow.deployments.BaseDeploymentClient(target_uri)[source] \\nBase class exposing Python model deployment APIs.\\nPlugin implementors should define target-specific deployment logic via a subclass of\\nBaseDeploymentClient within the plugin module, and customize method docstrings with\\ntarget-specific information.\\n\\nNote\\nSubclasses should raise mlflow.exceptions.MlflowException in error cases (e.g.\\non failure to deploy a model).\\n\\n\\n\\nabstract create_deployment(name, model_uri, flavor=None, config=None, endpoint=None)[source] \\nDeploy a model to the specified target. By default, this method should block until\\ndeployment completes (i.e. until it’s possible to perform inference with the deployment).\\nIn the case of conflicts (e.g. if it’s not possible to create the specified deployment\\nwithout due to conflict with an existing deployment), raises a\\nmlflow.exceptions.MlflowException or an HTTPError for remote\\ndeployments. See target-specific plugin documentation\\nfor additional detail on support for asynchronous deployment and other configuration.\\n\\nParameters\\n\\nname – Unique name to use for deployment. If another deployment exists with the same\\nname, raises a mlflow.exceptions.MlflowException\\nmodel_uri – URI of model to deploy\\nflavor – (optional) Model flavor to deploy. If unspecified, a default flavor\\nwill be chosen.\\nconfig – (optional) Dict containing updated target-specific configuration for the\\ndeployment\\nendpoint – (optional) Endpoint to create the deployment under. May not be supported\\nby all targets\\n\\n\\nReturns\\nDict corresponding to created deployment, which must contain the ‘name’ key.\\n\\n\\n\\n\\n\\ncreate_endpoint(name, config=None)[source] \\nCreate an endpoint with the specified target. By default, this method should block until\\ncreation completes (i.e. until it’s possible to create a deployment within the endpoint).\\nIn the case of conflicts (e.g. if it’s not possible to create the specified endpoint\\ndue to conflict with an existing endpoint), raises a\\nmlflow.exceptions.MlflowException or an HTTPError for remote\\ndeployments. See target-specific plugin documentation\\nfor additional detail on support for asynchronous creation and other configuration.\\n\\nParameters\\n\\nname – Unique name to use for endpoint. If another endpoint exists with the same\\nname, raises a mlflow.exceptions.MlflowException.\\nconfig – (optional) Dict containing target-specific configuration for the\\nendpoint.\\n\\n\\nReturns\\nDict corresponding to created endpoint, which must contain the ‘name’ key.\\n\\n\\n\\n\\n\\nabstract delete_deployment(name, config=None, endpoint=None)[source] \\nDelete the deployment with name name from the specified target.\\nDeletion should be idempotent (i.e. deletion should not fail if retried on a non-existent\\ndeployment).\\n\\nParameters\\n\\nname – Name of deployment to delete\\nconfig – (optional) dict containing updated target-specific configuration for the\\ndeployment\\nendpoint – (optional) Endpoint containing the deployment to delete. May not be\\nsupported by all targets\\n\\n\\nReturns\\nNone\\n\\n\\n\\n\\n\\ndelete_endpoint(endpoint)[source] \\nDelete the endpoint from the specified target. Deletion should be idempotent (i.e. deletion\\nshould not fail if retried on a non-existent deployment).\\n\\nParameters\\nendpoint – Name of endpoint to delete\\n\\nReturns\\nNone\\n\\n\\n\\n\\n\\nexplain(deployment_name=None, df=None, endpoint=None)[source] \\nGenerate explanations of model predictions on the specified input pandas Dataframe\\ndf for the deployed model. Explanation output formats vary by deployment target,\\nand can include details like feature importance for understanding/debugging predictions.\\n\\nParameters\\n\\ndeployment_name – Name of deployment to predict against\\ndf – Pandas DataFrame to use for explaining feature importance in model prediction\\nendpoint – Endpoint to predict against. May not be supported by all targets\\n\\n\\nReturns\\nA JSON-able object (pandas dataframe, numpy array, dictionary), or\\nan exception if the implementation is not available in deployment target’s class\\n\\n\\n\\n\\n\\nabstract get_deployment(name, endpoint=None)[source] \\nReturns a dictionary describing the specified deployment, throwing either a\\nmlflow.exceptions.MlflowException or an HTTPError for remote\\ndeployments if no deployment exists with the provided ID.\\nThe dict is guaranteed to contain an ‘name’ key containing the deployment name.\\nThe other fields of the returned dictionary and their types may vary across\\ndeployment targets.\\n\\nParameters\\n\\nname – ID of deployment to fetch.\\nendpoint – (optional) Endpoint containing the deployment to get. May not be\\nsupported by all targets.\\n\\n\\nReturns\\nA dict corresponding to the retrieved deployment. The dict is guaranteed to\\ncontain a ‘name’ key corresponding to the deployment name. The other fields of\\nthe returned dictionary and their types may vary across targets.\\n\\n\\n\\n\\n\\nget_endpoint(endpoint)[source] \\nReturns a dictionary describing the specified endpoint, throwing a\\npy:class:mlflow.exception.MlflowException or an HTTPError for remote\\ndeployments if no endpoint exists with the provided\\nname.\\nThe dict is guaranteed to contain an ‘name’ key containing the endpoint name.\\nThe other fields of the returned dictionary and their types may vary across targets.\\n\\nParameters\\nendpoint – Name of endpoint to fetch\\n\\nReturns\\nA dict corresponding to the retrieved endpoint. The dict is guaranteed to\\ncontain a ‘name’ key corresponding to the endpoint name. The other fields of\\nthe returned dictionary and their types may vary across targets.\\n\\n\\n\\n\\n\\nabstract list_deployments(endpoint=None)[source] \\nList deployments.\\nThis method is expected to return an unpaginated list of all\\ndeployments (an alternative would be to return a dict with a ‘deployments’ field\\ncontaining the actual deployments, with plugins able to specify other fields, e.g.\\na next_page_token field, in the returned dictionary for pagination, and to accept\\na pagination_args argument to this method for passing pagination-related args).\\n\\nParameters\\nendpoint – (optional) List deployments in the specified endpoint. May not be\\nsupported by all targets\\n\\nReturns\\nA list of dicts corresponding to deployments. Each dict is guaranteed to\\ncontain a ‘name’ key containing the deployment name. The other fields of\\nthe returned dictionary and their types may vary across deployment targets.\\n\\n\\n\\n\\n\\nlist_endpoints()[source] \\nList endpoints in the specified target. This method is expected to return an\\nunpaginated list of all endpoints (an alternative would be to return a dict with\\nan ‘endpoints’ field containing the actual endpoints, with plugins able to specify\\nother fields, e.g. a next_page_token field, in the returned dictionary for pagination,\\nand to accept a pagination_args argument to this method for passing\\npagination-related args).\\n\\nReturns\\nA list of dicts corresponding to endpoints. Each dict is guaranteed to\\ncontain a ‘name’ key containing the endpoint name. The other fields of\\nthe returned dictionary and their types may vary across targets.\\n\\n\\n\\n\\n\\nabstract predict(deployment_name=None, inputs=None, endpoint=None)[source] \\nCompute predictions on inputs using the specified deployment or model endpoint.\\nNote that the input/output types of this method match those of mlflow pyfunc predict.\\n\\nParameters\\n\\ndeployment_name – Name of deployment to predict against.\\ninputs – Input data (or arguments) to pass to the deployment or model endpoint for\\ninference.\\nendpoint – Endpoint to predict against. May not be supported by all targets.\\n\\n\\nReturns\\nA mlflow.deployments.PredictionsResponse instance representing the\\npredictions and associated Model Server response metadata.\\n\\n\\n\\n\\n\\npredict_stream(deployment_name=None, inputs=None, endpoint=None)[source] \\nSubmit a query to a configured provider endpoint, and get streaming response\\n\\nParameters\\n\\ndeployment_name – Name of deployment to predict against.\\ninputs – The inputs to the query, as a dictionary.\\nendpoint – The name of the endpoint to query.\\n\\n\\nReturns\\nAn iterator of dictionary containing the response from the endpoint.\\n\\n\\n\\n\\n\\nabstract update_deployment(name, model_uri=None, flavor=None, config=None, endpoint=None)[source] \\nUpdate the deployment with the specified name. You can update the URI of the model, the\\nflavor of the deployed model (in which case the model URI must also be specified), and/or\\nany target-specific attributes of the deployment (via config). By default, this method\\nshould block until deployment completes (i.e. until it’s possible to perform inference\\nwith the updated deployment). See target-specific plugin documentation for additional\\ndetail on support for asynchronous deployment and other configuration.\\n\\nParameters\\n\\nname – Unique name of deployment to update.\\nmodel_uri – URI of a new model to deploy.\\nflavor – (optional) new model flavor to use for deployment. If provided,\\nmodel_uri must also be specified. If flavor is unspecified but\\nmodel_uri is specified, a default flavor will be chosen and the\\ndeployment will be updated using that flavor.\\nconfig – (optional) dict containing updated target-specific configuration for the\\ndeployment.\\nendpoint – (optional) Endpoint containing the deployment to update. May not be\\nsupported by all targets.\\n\\n\\nReturns\\nNone\\n\\n\\n\\n\\n\\nupdate_endpoint(endpoint, config=None)[source] \\nUpdate the endpoint with the specified name. You can update any target-specific attributes\\nof the endpoint (via config). By default, this method should block until the update\\ncompletes (i.e. until it’s possible to create a deployment within the endpoint). See\\ntarget-specific plugin documentation for additional detail on support for asynchronous\\nupdate and other configuration.\\n\\nParameters\\n\\nendpoint – Unique name of endpoint to update\\nconfig – (optional) dict containing target-specific configuration for the\\nendpoint\\n\\n\\nReturns\\nNone\\n\\n\\n\\n\\n\\n\\nclass mlflow.deployments.DatabricksDeploymentClient(target_uri)[source] \\n\\nNote\\nExperimental: This class may change or be removed in a future release without warning.\\n\\nClient for interacting with Databricks serving endpoints.\\nExample:\\nFirst, set up credentials for authentication:\\nexport DATABRICKS_HOST=...\\nexport DATABRICKS_TOKEN=...\\n\\n\\n\\nSee also\\nSee https://docs.databricks.com/en/dev-tools/auth.html for other authentication methods.\\n\\nThen, create a deployment client and use it to interact with Databricks serving endpoints:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nendpoints = client.list_endpoints()\\nassert endpoints == [\\n    {\\n        \"name\": \"chat\",\\n        \"creator\": \"alice@company.com\",\\n        \"creation_timestamp\": 0,\\n        \"last_updated_timestamp\": 0,\\n        \"state\": {...},\\n        \"config\": {...},\\n        \"tags\": [...],\\n        \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n    },\\n]\\n\\n\\n\\n\\ncreate_deployment(name, model_uri, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for DatabricksDeploymentClient.\\n\\n\\n\\n\\ncreate_endpoint(name, config=None, route_optimized=False)[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nCreate a new serving endpoint with the provided name and configuration.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/create for request/response\\nschema.\\n\\nParameters\\n\\nname – The name of the serving endpoint to create.\\nconfig – A dictionary containing the configuration of the serving endpoint to create.\\nroute_optimized – A boolean which defines whether databricks serving endpoint\\nin optimized for routing traffic. Refer to the following doc for more details.\\nhttps://docs.databricks.com/en/machine-learning/model-serving/route-optimization.html#enable-route-optimization-on-a-feature-serving-endpoint\\n\\n\\nReturns\\nA DatabricksEndpoint object containing the request response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nendpoint = client.create_endpoint(\\n    name=\"chat\",\\n    config={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-4\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/chat\",\\n                    \"openai_config\": {\\n                        \"openai_api_key\": \"{{secrets/scope/key}}\",\\n                    },\\n                },\\n            }\\n        ],\\n    },\\n    route_optimized: True\\n)\\nassert endpoint == {\\n    \"name\": \"chat\",\\n    \"creator\": \"alice@company.com\",\\n    \"creation_timestamp\": 0,\\n    \"last_updated_timestamp\": 0,\\n    \"state\": {...},\\n    \"config\": {...},\\n    \"tags\": [...],\\n    \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n}\\n\\n\\n\\n\\n\\ndelete_deployment(name, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for DatabricksDeploymentClient.\\n\\n\\n\\n\\ndelete_endpoint(endpoint)[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nDelete a specified serving endpoint.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/delete for request/response\\nschema.\\n\\nParameters\\nendpoint – The name of the serving endpoint to delete.\\n\\nReturns\\nA DatabricksEndpoint object containing the request response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nclient.delete_endpoint(endpoint=\"chat\")\\n\\n\\n\\n\\n\\nget_deployment(name, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for DatabricksDeploymentClient.\\n\\n\\n\\n\\nget_endpoint(endpoint)[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nGet a specified serving endpoint.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/get for request/response\\nschema.\\n\\nParameters\\nendpoint – The name of the serving endpoint to get.\\n\\nReturns\\nA DatabricksEndpoint object containing the request response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nendpoint = client.get_endpoint(endpoint=\"chat\")\\nassert endpoint == {\\n    \"name\": \"chat\",\\n    \"creator\": \"alice@company.com\",\\n    \"creation_timestamp\": 0,\\n    \"last_updated_timestamp\": 0,\\n    \"state\": {...},\\n    \"config\": {...},\\n    \"tags\": [...],\\n    \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n}\\n\\n\\n\\n\\n\\nlist_deployments(endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for DatabricksDeploymentClient.\\n\\n\\n\\n\\nlist_endpoints()[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nRetrieve all serving endpoints.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/list for request/response\\nschema.\\n\\nReturns\\nA list of DatabricksEndpoint objects containing the request response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nendpoints = client.list_endpoints()\\nassert endpoints == [\\n    {\\n        \"name\": \"chat\",\\n        \"creator\": \"alice@company.com\",\\n        \"creation_timestamp\": 0,\\n        \"last_updated_timestamp\": 0,\\n        \"state\": {...},\\n        \"config\": {...},\\n        \"tags\": [...],\\n        \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n    },\\n]\\n\\n\\n\\n\\n\\npredict(deployment_name=None, inputs=None, endpoint=None)[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nQuery a serving endpoint with the provided model inputs.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/query for request/response\\nschema.\\n\\nParameters\\n\\ndeployment_name – Unused.\\ninputs – A dictionary containing the model inputs to query.\\nendpoint – The name of the serving endpoint to query.\\n\\n\\nReturns\\nA DatabricksEndpoint object containing the query response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nresponse = client.predict(\\n    endpoint=\"chat\",\\n    inputs={\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"Hello!\"},\\n        ],\\n    },\\n)\\nassert response == {\\n    \"id\": \"chatcmpl-8OLm5kfqBAJD8CpsMANESWKpLSLXY\",\\n    \"object\": \"chat.completion\",\\n    \"created\": 1700814265,\\n    \"model\": \"gpt-4-0613\",\\n    \"choices\": [\\n        {\\n            \"index\": 0,\\n            \"message\": {\\n                \"role\": \"assistant\",\\n                \"content\": \"Hello! How can I assist you today?\",\\n            },\\n            \"finish_reason\": \"stop\",\\n        }\\n    ],\\n    \"usage\": {\\n        \"prompt_tokens\": 9,\\n        \"completion_tokens\": 9,\\n        \"total_tokens\": 18,\\n    },\\n}\\n\\n\\n\\n\\n\\npredict_stream(deployment_name=None, inputs=None, endpoint=None) → Iterator[Dict[str, Any]][source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nSubmit a query to a configured provider endpoint, and get streaming response\\n\\nParameters\\n\\ndeployment_name – Unused.\\ninputs – The inputs to the query, as a dictionary.\\nendpoint – The name of the endpoint to query.\\n\\n\\nReturns\\nAn iterator of dictionary containing the response from the endpoint.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nchunk_iter = client.predict_stream(\\n    endpoint=\"databricks-llama-2-70b-chat\",\\n    inputs={\\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\\n        \"temperature\": 0.0,\\n        \"n\": 1,\\n        \"max_tokens\": 500,\\n    },\\n)\\nfor chunk in chunk_iter:\\n    print(chunk)\\n    # Example:\\n    # {\\n    #     \"id\": \"82a834f5-089d-4fc0-ad6c-db5c7d6a6129\",\\n    #     \"object\": \"chat.completion.chunk\",\\n    #     \"created\": 1712133837,\\n    #     \"model\": \"llama-2-70b-chat-030424\",\\n    #     \"choices\": [\\n    #         {\\n    #             \"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"Hello\"},\\n    #             \"finish_reason\": None,\\n    #         }\\n    #     ],\\n    #     \"usage\": {\"prompt_tokens\": 11, \"completion_tokens\": 1, \"total_tokens\": 12},\\n    # }\\n\\n\\n\\n\\n\\nupdate_deployment(name, model_uri=None, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for DatabricksDeploymentClient.\\n\\n\\n\\n\\nupdate_endpoint(endpoint, config=None)[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nUpdate a specified serving endpoint with the provided configuration.\\nSee https://docs.databricks.com/api/workspace/servingendpoints/updateconfig for\\nrequest/response schema.\\n\\nParameters\\n\\nendpoint – The name of the serving endpoint to update.\\nconfig – A dictionary containing the configuration of the serving endpoint to update.\\n\\n\\nReturns\\nA DatabricksEndpoint object containing the request response.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"databricks\")\\nendpoint = client.update_endpoint(\\n    endpoint=\"chat\",\\n    config={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-4\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/chat\",\\n                    \"openai_config\": {\\n                        \"openai_api_key\": \"{{secrets/scope/key}}\",\\n                    },\\n                },\\n            }\\n        ],\\n    },\\n)\\nassert endpoint == {\\n    \"name\": \"chat\",\\n    \"creator\": \"alice@company.com\",\\n    \"creation_timestamp\": 0,\\n    \"last_updated_timestamp\": 0,\\n    \"state\": {...},\\n    \"config\": {...},\\n    \"tags\": [...],\\n    \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n}\\n\\nrate_limits = client.update_endpoint(\\n    endpoint=\"chat\",\\n    config={\\n        \"rate_limits\": [\\n            {\\n                \"key\": \"user\",\\n                \"renewal_period\": \"minute\",\\n                \"calls\": 10,\\n            }\\n        ],\\n    },\\n)\\nassert rate_limits == {\\n    \"rate_limits\": [\\n        {\\n            \"key\": \"user\",\\n            \"renewal_period\": \"minute\",\\n            \"calls\": 10,\\n        }\\n    ],\\n}\\n\\n\\n\\n\\n\\n\\nclass mlflow.deployments.DatabricksEndpoint[source] \\nA dictionary-like object representing a Databricks serving endpoint.\\nendpoint = DatabricksEndpoint(\\n    {\\n        \"name\": \"chat\",\\n        \"creator\": \"alice@company.com\",\\n        \"creation_timestamp\": 0,\\n        \"last_updated_timestamp\": 0,\\n        \"state\": {...},\\n        \"config\": {...},\\n        \"tags\": [...],\\n        \"id\": \"88fd3f75a0d24b0380ddc40484d7a31b\",\\n    }\\n)\\nassert endpoint.name == \"chat\"\\n\\n\\n\\n\\n\\nclass mlflow.deployments.MlflowDeploymentClient(target_uri)[source] \\n\\nNote\\nExperimental: This class may change or be removed in a future release without warning.\\n\\nClient for interacting with the MLflow AI Gateway.\\nExample:\\nFirst, start the MLflow AI Gateway:\\nmlflow gateway start --config-path path/to/config.yaml\\n\\n\\nThen, create a client and use it to interact with the server:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"http://localhost:5000\")\\nendpoints = client.list_endpoints()\\nassert [e.dict() for e in endpoints] == [\\n    {\\n        \"name\": \"chat\",\\n        \"endpoint_type\": \"llm/v1/chat\",\\n        \"model\": {\"name\": \"gpt-4o-mini\", \"provider\": \"openai\"},\\n        \"endpoint_url\": \"http://localhost:5000/gateway/chat/invocations\",\\n    },\\n]\\n\\n\\n\\n\\ncreate_deployment(name, model_uri, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\ncreate_endpoint(name, config=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\ndelete_deployment(name, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\ndelete_endpoint(endpoint)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\nget_deployment(name, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for MLflowDeploymentClient.\\n\\n\\n\\n\\nget_endpoint(endpoint) → Endpoint[source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nGets a specified endpoint configured for the MLflow AI Gateway.\\n\\nParameters\\nendpoint – The name of the endpoint to retrieve.\\n\\nReturns\\nAn Endpoint object representing the endpoint.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"http://localhost:5000\")\\nendpoint = client.get_endpoint(endpoint=\"chat\")\\nassert endpoint.dict() == {\\n    \"name\": \"chat\",\\n    \"endpoint_type\": \"llm/v1/chat\",\\n    \"model\": {\"name\": \"gpt-4o-mini\", \"provider\": \"openai\"},\\n    \"endpoint_url\": \"http://localhost:5000/gateway/chat/invocations\",\\n}\\n\\n\\n\\n\\n\\nlist_deployments(endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\nlist_endpoints() → List[Endpoint][source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nList endpoints configured for the MLflow AI Gateway.\\n\\nReturns\\nA list of Endpoint objects.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"http://localhost:5000\")\\n\\nendpoints = client.list_endpoints()\\nassert [e.dict() for e in endpoints] == [\\n    {\\n        \"name\": \"chat\",\\n        \"endpoint_type\": \"llm/v1/chat\",\\n        \"model\": {\"name\": \"gpt-4o-mini\", \"provider\": \"openai\"},\\n        \"endpoint_url\": \"http://localhost:5000/gateway/chat/invocations\",\\n    },\\n]\\n\\n\\n\\n\\n\\npredict(deployment_name=None, inputs=None, endpoint=None) → Dict[str, Any][source] \\n\\nNote\\nExperimental: This function may change or be removed in a future release without warning.\\n\\nSubmit a query to a configured provider endpoint.\\n\\nParameters\\n\\ndeployment_name – Unused.\\ninputs – The inputs to the query, as a dictionary.\\nendpoint – The name of the endpoint to query.\\n\\n\\nReturns\\nA dictionary containing the response from the endpoint.\\n\\n\\nExample:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"http://localhost:5000\")\\n\\nresponse = client.predict(\\n    endpoint=\"chat\",\\n    inputs={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\\n)\\nassert response == {\\n    \"id\": \"chatcmpl-8OLoQuaeJSLybq3NBoe0w5eyqjGb9\",\\n    \"object\": \"chat.completion\",\\n    \"created\": 1700814410,\\n    \"model\": \"gpt-4o-mini\",\\n    \"choices\": [\\n        {\\n            \"index\": 0,\\n            \"message\": {\\n                \"role\": \"assistant\",\\n                \"content\": \"Hello! How can I assist you today?\",\\n            },\\n            \"finish_reason\": \"stop\",\\n        }\\n    ],\\n    \"usage\": {\\n        \"prompt_tokens\": 9,\\n        \"completion_tokens\": 9,\\n        \"total_tokens\": 18,\\n    },\\n}\\n\\n\\nAdditional parameters that are valid for a given provider and endpoint configuration can be\\nincluded with the request as shown below, using an openai completions endpoint request as\\nan example:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"http://localhost:5000\")\\nclient.predict(\\n    endpoint=\"completions\",\\n    inputs={\\n        \"prompt\": \"Hello!\",\\n        \"temperature\": 0.3,\\n        \"max_tokens\": 500,\\n    },\\n)\\n\\n\\n\\n\\n\\nupdate_deployment(name, model_uri=None, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\nupdate_endpoint(endpoint, config=None)[source] \\n\\nWarning\\nThis method is not implemented for MlflowDeploymentClient.\\n\\n\\n\\n\\n\\nclass mlflow.deployments.OpenAIDeploymentClient(target_uri)[source] \\nClient for interacting with OpenAI endpoints.\\nExample:\\nFirst, set up credentials for authentication:\\nexport OPENAI_API_KEY=...\\n\\n\\n\\nSee also\\nSee https://mlflow.org/docs/latest/python_api/openai/index.html for other authentication\\nmethods.\\n\\nThen, create a deployment client and use it to interact with OpenAI endpoints:\\nfrom mlflow.deployments import get_deploy_client\\n\\nclient = get_deploy_client(\"openai\")\\nclient.predict(\\n    endpoint=\"gpt-4o-mini\",\\n    inputs={\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"Hello!\"},\\n        ],\\n    },\\n)\\n\\n\\n\\n\\ncreate_deployment(name, model_uri, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\ncreate_endpoint(name, config=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\ndelete_deployment(name, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\ndelete_endpoint(endpoint)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\nget_deployment(name, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\nget_endpoint(endpoint)[source] \\nGet information about a specific model.\\n\\n\\n\\nlist_deployments(endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\nlist_endpoints()[source] \\nList the currently available models.\\n\\n\\n\\npredict(deployment_name=None, inputs=None, endpoint=None)[source] \\nQuery an OpenAI endpoint.\\nSee https://platform.openai.com/docs/api-reference for more information.\\n\\nParameters\\n\\ndeployment_name – Unused.\\ninputs – A dictionary containing the model inputs to query.\\nendpoint – The name of the endpoint to query.\\n\\n\\nReturns\\nA dictionary containing the model outputs.\\n\\n\\n\\n\\n\\nupdate_deployment(name, model_uri=None, flavor=None, config=None, endpoint=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\nupdate_endpoint(endpoint, config=None)[source] \\n\\nWarning\\nThis method is not implemented for OpenAIDeploymentClient.\\n\\n\\n\\n\\n\\nmlflow.deployments.get_deploy_client(target_uri=None)[source] \\nReturns a subclass of mlflow.deployments.BaseDeploymentClient exposing standard\\nAPIs for deploying models to the specified target. See available deployment APIs\\nby calling help() on the returned object or viewing docs for\\nmlflow.deployments.BaseDeploymentClient. You can also run\\nmlflow deployments help -t <target-uri> via the CLI for more details on target-specific\\nconfiguration options.\\n\\nParameters\\ntarget_uri – Optional URI of target to deploy to. If no target URI is provided, then\\nMLflow will attempt to get the deployments target set via get_deployments_target() or\\nMLFLOW_DEPLOYMENTS_TARGET environment variable.\\n\\n\\n\\nExample \\nfrom mlflow.deployments import get_deploy_client\\nimport pandas as pd\\n\\nclient = get_deploy_client(\"redisai\")\\n# Deploy the model stored at artifact path \\'myModel\\' under run with ID \\'someRunId\\'. The\\n# model artifacts are fetched from the current tracking server and then used for deployment.\\nclient.create_deployment(\"spamDetector\", \"runs:/someRunId/myModel\")\\n# Load a CSV of emails and score it against our deployment\\nemails_df = pd.read_csv(\"...\")\\nprediction_df = client.predict_deployment(\"spamDetector\", emails_df)\\n# List all deployments, get details of our particular deployment\\nprint(client.list_deployments())\\nprint(client.get_deployment(\"spamDetector\"))\\n# Update our deployment to serve a different model\\nclient.update_deployment(\"spamDetector\", \"runs:/anotherRunId/myModel\")\\n# Delete our deployment\\nclient.delete_deployment(\"spamDetector\")\\n\\n\\n\\n\\n\\n\\nmlflow.deployments.get_deployments_target() → str[source] \\nReturns the currently set MLflow deployments target iff set.\\nIf the deployments target has not been set by using set_deployments_target, an\\nMlflowException is raised.\\n\\n\\n\\nmlflow.deployments.run_local(target, name, model_uri, flavor=None, config=None)[source] \\nDeploys the specified model locally, for testing. Note that models deployed locally cannot\\nbe managed by other deployment APIs (e.g. update_deployment, delete_deployment, etc).\\n\\nParameters\\n\\ntarget – Target to deploy to.\\nname – Name to use for deployment\\nmodel_uri – URI of model to deploy\\nflavor – (optional) Model flavor to deploy. If unspecified, a default flavor\\nwill be chosen.\\nconfig – (optional) Dict containing updated target-specific configuration for\\nthe deployment\\n\\n\\nReturns\\nNone\\n\\n\\n\\n\\n\\nmlflow.deployments.set_deployments_target(target: str)[source] \\nSets the target deployment client for MLflow deployments\\n\\nParameters\\ntarget – The full uri of a running MLflow AI Gateway or, if running on\\nDatabricks, “databricks”.\\n\\n\\n\\n\\n\\nclass mlflow.deployments.PredictionsResponse[source] \\nRepresents the predictions and metadata returned in response to a scoring request, such as a\\nREST API request sent to the /invocations endpoint of an MLflow Model Server.\\n\\n\\nget_predictions(predictions_format=\\'dataframe\\', dtype=None)[source] \\nGet the predictions returned from the MLflow Model Server in the specified format.\\n\\nParameters\\n\\npredictions_format – The format in which to return the predictions. Either\\n\"dataframe\" or \"ndarray\".\\ndtype – The NumPy datatype to which to coerce the predictions. Only used when\\nthe “ndarray” predictions_format is specified.\\n\\n\\nRaises\\nException – If the predictions cannot be represented in the specified format.\\n\\nReturns\\nThe predictions, represented in the specified format.\\n\\n\\n\\n\\n\\nto_json(path=None)[source] \\nGet the JSON representation of the MLflow Predictions Response.\\n\\nParameters\\npath – If specified, the JSON representation is written to this file path.\\n\\nReturns\\nIf path is unspecified, the JSON representation of the MLflow Predictions\\nResponse. Else, None.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n\\n          © MLflow Project, a Series of LF Projects, LLC. All rights reserved.\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://mlflow.org/docs/latest/index.html\",\n",
    "        \"https://mlflow.org/docs/latest/tracking/autolog.html\",\n",
    "        \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\n",
    "        \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\",\n",
    "    ]\n",
    ")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a7e77e-6717-472a-86dc-02e2c356ddef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 745, which is longer than the specified 500\n",
      "Created a chunk of size 567, which is longer than the specified 500\n",
      "Created a chunk of size 583, which is longer than the specified 500\n",
      "Created a chunk of size 670, which is longer than the specified 500\n",
      "Created a chunk of size 841, which is longer than the specified 500\n",
      "Created a chunk of size 1133, which is longer than the specified 500\n",
      "Created a chunk of size 629, which is longer than the specified 500\n",
      "Created a chunk of size 617, which is longer than the specified 500\n",
      "Created a chunk of size 559, which is longer than the specified 500\n",
      "Created a chunk of size 607, which is longer than the specified 500\n",
      "Created a chunk of size 530, which is longer than the specified 500\n",
      "Created a chunk of size 793, which is longer than the specified 500\n",
      "Created a chunk of size 713, which is longer than the specified 500\n",
      "Created a chunk of size 804, which is longer than the specified 500\n",
      "Created a chunk of size 771, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 775, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted into 155 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['03e7d370-e419-4c87-8b7f-c3bc9f3caf52',\n",
       " 'e0cc3e67-9fec-4fdf-bce7-5b2ea6ee6c03',\n",
       " '732407c9-d446-4096-8821-a4ea300d6b79',\n",
       " 'be0b25de-5b93-4da4-9dd5-5b240f705ab1',\n",
       " 'fa9fe616-8530-4320-8203-92bd671aad2b',\n",
       " '98d5f5e3-75f3-420a-b9c8-643ebe0eb646',\n",
       " '24de5746-13af-4c87-90b2-eef82d86e70b',\n",
       " 'cfb4fda6-06e3-4d71-b56f-c26a66143a66',\n",
       " '49c44a41-8518-4d62-be55-d9e7d62c748f',\n",
       " '46811949-d8f4-41f1-bc2e-608d5abb691a',\n",
       " 'f9f030da-9f0e-4ac9-bd41-f479ec3225fb',\n",
       " '01905bbf-a4c9-46cf-9e71-ccd6b0747102',\n",
       " '7e6760fd-5b77-48e7-abb4-ba9909abdac9',\n",
       " '1f257387-1f02-473f-91d1-99272f8c1227',\n",
       " '3e2e487c-a7b4-4500-918b-d8406e4ceca5',\n",
       " '08b00e1e-502e-4648-a972-231f4b60dfb0',\n",
       " '9978340b-6566-4f64-80e5-5189ac640905',\n",
       " '1390da84-cc13-430e-8ae2-528190a3777a',\n",
       " '0011cde9-bdcd-475d-9d6e-f48f4239aefc',\n",
       " 'f2891cbb-0a24-4eff-a1c6-29651c09996c',\n",
       " 'e22679a6-d301-4527-9b29-e7bc020171e8',\n",
       " '8a760f94-5020-4fd8-a900-e4ca5b720918',\n",
       " '89151717-f249-43c2-87df-e1c91a42b0f0',\n",
       " '1348ee8f-f26c-4fbf-94cb-1385584c1d2b',\n",
       " '55ecc44d-f36c-4f2b-a33e-a53ca0e9ecc9',\n",
       " '54d66d03-1118-4665-b4f9-48a68224c1a9',\n",
       " 'ca499bf9-4d47-4be8-a273-33a1c5eccbd1',\n",
       " '29cb03d3-69c0-4859-b52d-77d9dbab59e2',\n",
       " 'fce45417-047c-4b39-8e4d-c060521bf2a2',\n",
       " '02d146a4-9d3e-443d-bcc7-d6da3a5786f6',\n",
       " '56a30974-61da-49cd-ad65-2be63723d467',\n",
       " 'b28816b9-7f2c-408b-a5fc-9fabaaed88b7',\n",
       " '49d6bf5a-56ed-4c9c-becd-0b7c2fe9a1be',\n",
       " '3c19125e-5f01-4d9e-9728-1696bc6f47c4',\n",
       " 'ebf056a4-520c-403d-b61e-13283e19f32b',\n",
       " 'ad770158-8d3c-462f-81f8-f9f98d53c381',\n",
       " 'f778c486-6153-45a7-aa86-ee2b6c4af1b8',\n",
       " '57109dad-90c5-476d-9c59-ca97308b233f',\n",
       " '05ad7ec5-4a36-485e-b312-e263678729db',\n",
       " 'b4210636-4d68-46e7-9a98-676291a32732',\n",
       " '12e54fa4-16d1-43aa-842b-4802a5c6f42e',\n",
       " 'c8283b6e-a359-4fe1-a272-973217c7df32',\n",
       " '4505d2d4-8b5f-4c5b-a631-fa7d837e4771',\n",
       " 'ef9af92b-86c6-4164-9fdc-569f0c9ee418',\n",
       " 'ad4b5672-31b5-4cb3-ae44-da1a0e3bec95',\n",
       " '99addd2d-d6c2-45c6-9ed3-d379d1dbfe43',\n",
       " 'fdb936e8-3e87-44a6-8a61-eab03ee1e39e',\n",
       " '941735fe-c792-4b76-a2b2-05480fe8890d',\n",
       " 'ebe30b5d-44c3-43e0-ba55-b366c9838188',\n",
       " 'bc29a524-b5d2-45b8-8045-714801956da5',\n",
       " '9ca9ffae-af03-4860-bc01-3af9ca56f235',\n",
       " 'baf25bb4-3dbf-473b-92b3-64bd9c622c1b',\n",
       " '1ba10a67-a83c-45f6-bc35-b7435da17cc9',\n",
       " '8fdec043-c58d-462b-9928-f5838e46238b',\n",
       " '4f324699-df67-4530-94b0-9146d51cca1d',\n",
       " '220e9d27-9090-461b-9860-77706e954f2b',\n",
       " 'af37c307-610f-40e1-9d9c-57c88bd0d12d',\n",
       " 'a0497118-bdc6-47ee-8c6a-b8be48897f39',\n",
       " 'd6cdb4bf-6e0c-4a65-8f6c-f9d5f9c54512',\n",
       " 'ae6a1f75-3963-4732-a72e-b77ffcd48778',\n",
       " '81eaaa8b-c77f-4e3f-ae72-d909e22eed19',\n",
       " 'e84afd98-dbda-43ba-8edf-a8da93b5a7ea',\n",
       " '17e06a5f-9834-49f1-b247-479646dd4a87',\n",
       " '6b6a8266-f8af-44cc-bf01-a109ebef4108',\n",
       " '2094662e-e306-4b1f-a414-1348b12dde06',\n",
       " 'baca0b03-0c1c-4da5-af9c-90e7b4fe526b',\n",
       " '0cddbf9f-7b97-42ba-81b6-5a6dd4f0b5e0',\n",
       " '3ac163f3-367a-43b3-be3e-e0bdc0f5e66c',\n",
       " 'd11c7b68-c9dd-4dcd-80d3-342a5108335e',\n",
       " 'e40acf8e-a7c8-4745-bdb8-4d7993c7e51d',\n",
       " '021efd3f-87ef-4793-82e9-d298976d206e',\n",
       " '7e998aad-ae07-4b5a-969b-77a80a348967',\n",
       " 'c52a9ec5-9ee2-48d2-984f-a3c748de3177',\n",
       " '6067baad-bc08-43e5-a7f0-994dc2d75a21',\n",
       " '64e01443-0ee3-41e1-b560-0990636528b8',\n",
       " 'b267a7bb-2636-4e00-b329-bfaf71019fc2',\n",
       " 'df166ffc-1fbe-46d6-92de-24c2ebc510de',\n",
       " '47681a48-aea5-4712-aa03-5a544075db33',\n",
       " '22d648f4-3d09-4767-84cd-eb418761d32c',\n",
       " 'd15fd922-d8bf-4cf9-8591-8e3a18c86d68',\n",
       " '02ac241a-424d-4b16-85d7-f7f99664042e',\n",
       " '7a465800-b99d-42aa-9c21-7057899fbd09',\n",
       " '4716e1c5-fbd1-49b9-9bf8-966f22461ad6',\n",
       " '426b64dc-9adb-4c6b-bfbb-2aa14a04999f',\n",
       " 'e5df004e-81c7-41c7-be8e-24392def71cc',\n",
       " 'b069291b-45c9-4f9e-951e-7197260792c5',\n",
       " '9a40f4b3-fb84-418d-b8e4-2b1c058ffcb5',\n",
       " 'c716f81d-e772-4095-83f1-a743dcb4ba66',\n",
       " '387bf2b7-05af-495a-8622-384455eab659',\n",
       " '07cf3d28-297f-4786-9aa3-c828d637c72e',\n",
       " 'f2d43826-388d-4330-a39e-c2e134ed2d1c',\n",
       " 'bb22bb8e-3cbc-4191-8afe-788a1539ccb8',\n",
       " '5470f514-bc52-4804-82a4-c2269a4ee77a',\n",
       " '5be33e7d-e806-4638-b9fd-4cafa7817665',\n",
       " 'ec4ea926-4b34-41e0-bfb6-7b6bc209eb0f',\n",
       " '84f97dce-5468-4f9a-b3a4-53979a05169e',\n",
       " '77ab7c9b-6c48-47e0-8610-3a28925dd4ed',\n",
       " '0bb29772-d2eb-4df9-85dc-bbbdf85d55c0',\n",
       " '0ed3f7e0-91a4-47fe-9b39-a1b167c3fb79',\n",
       " '6acf083c-3193-4b4c-b371-0ce44ca75ab1',\n",
       " '86098a3e-863c-4028-8510-c86e4a2ef8d0',\n",
       " 'cd02ccff-e4ac-490b-bd3c-520cbc554c18',\n",
       " 'ac3878d2-57f4-4c37-9f3b-2d0095339a45',\n",
       " '8fbd728f-a457-4cce-84de-68885d115d37',\n",
       " '1bca8acc-a062-4ff8-8fa1-8e05f50b6c5a',\n",
       " '9ef8358e-87e6-485e-acc0-5fba78f58b67',\n",
       " 'b4d68f44-5700-4582-8803-5b56812f8ad8',\n",
       " '5d2cc17d-0095-4aa0-afeb-1d1f6a1a215f',\n",
       " '45f397b7-41b4-4ad6-8525-231e2da7f75f',\n",
       " 'd40ccdea-3a88-4521-a96a-a95c8630e4c0',\n",
       " 'd9b923b1-87c0-43ec-b5cf-d9e472158c36',\n",
       " '55fcaee3-31d2-4f76-9d33-2be50b490279',\n",
       " '77ed2381-3ea4-4d91-8c8a-84bd0feb65f8',\n",
       " '604e95e6-7f8b-498d-8380-b081f9e680ad',\n",
       " 'f1719075-2e65-48fa-bc0f-7a90d542c4e9',\n",
       " 'bb254149-77d6-414a-88b1-af5c7ab48d72',\n",
       " '15417920-c9c8-4af6-b87e-cfb4ca2ca065',\n",
       " 'd42d6b69-c96b-4643-a514-90b47679de36',\n",
       " 'e0baac3c-b58c-4d70-bb01-a9cc08e03597',\n",
       " 'adcb1784-328c-4381-814e-23592492e860',\n",
       " 'ab85ad07-45a5-4322-b60a-4dd1e7726e97',\n",
       " '9cb3ad63-08f5-496a-bb48-622b3bf2d657',\n",
       " '5a2458d5-1d26-4b9f-a6e3-64d8fa21436d',\n",
       " '777b0b0a-e39f-4d83-a3ef-e9690c90914a',\n",
       " '4da79797-a939-4450-b080-372ac0876e6a',\n",
       " 'f24b8970-b629-4c13-9bd8-748c01847536',\n",
       " '9dace949-e544-4a0a-9d94-3a7dde90b344',\n",
       " 'c7c80fd3-f6f5-43f2-810e-efadfe8b9d0e',\n",
       " '4848f2b0-224f-4b3e-a7b5-d909d39b70fa',\n",
       " '10674693-f506-4c6a-aa08-fb0164126a58',\n",
       " 'cf81db5d-7e92-45a5-aa20-576c1d86e03b',\n",
       " '8d0fcc9d-9a79-48a7-a4f1-200e3f0bfece',\n",
       " '162ca8c4-07ad-4ed2-9e6b-1c752564cb30',\n",
       " '52157154-71d7-4114-8f48-c78f76da7745',\n",
       " '2b5a162f-723a-43d1-b859-f13b261eb67a',\n",
       " 'fc6c48d0-8a19-4e1b-b2dc-2d511d47abfc',\n",
       " 'f8b4d42b-7440-4a26-8d4f-00c93e968021',\n",
       " '31b285d0-15eb-46bd-95ba-516b262e3aa7',\n",
       " '16fa5f64-fa9b-4589-9868-ba18c9bfa876',\n",
       " '72499e56-225d-4c8f-8a97-1989f13c5a8f',\n",
       " 'e2984f8d-e18f-42c3-a409-97dce0320f9d',\n",
       " 'a923d0dc-246e-4564-8a11-2f3b6a53200f',\n",
       " 'ae855ba8-e9bc-4237-a1cd-5f6825a355c4',\n",
       " '423fa78e-7222-4b24-998f-7261cf7a82b3',\n",
       " '41706b7e-b0b0-47f4-b05b-88cd7eed9114',\n",
       " 'fde009e1-9532-4128-8264-8643bc66dbf2',\n",
       " '3ff6c68c-5df8-4542-842a-8bcb3e5e0fd8',\n",
       " '7d621be4-9b18-49b3-9d66-b1342d848e9a',\n",
       " 'cf37936e-239b-45fc-882d-650e53f781b1',\n",
       " '88af05f3-7a87-47c3-a2c0-0007ea81351d',\n",
       " '2be7571a-9ad4-4602-a6ad-ca3dbfabe0d2',\n",
       " '6fd20131-4944-4c3b-b8c6-11cf823bdf55',\n",
       " '2bf3ce26-df64-4559-a20d-ef94190f0ab9',\n",
       " '3329ff32-1a03-4535-b7c6-0dc6aaae460e',\n",
       " '1f42b767-16c9-47c8-b580-2f3a187cb977']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk the documents into smaller pieces\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Splitted into {len(texts)} documents\")\n",
    "\n",
    "# Initialize the components of the chain\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client, model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Add the documents to the vector store\n",
    "vector_store.add_documents(documents=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build qa chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'https://mlflow.org/docs/latest/index.html', 'title': 'MLflow: A Tool for Managing the Machine Learning Lifecycle', 'language': 'en'}, page_content='Learn about MLflowMLflow BasicsMLflow Models IntroductionGenAI QuickstartsDeep Learning Quickstarts\\nLearn about the core components of MLflow\\n\\n\\nQuickstarts\\n\\n                Get Started with MLflow in our 5-minute tutorial\\n\\nGuides\\n\\n                Learn the core components of MLflow with this in-depth guide to Tracking\\n\\n\\nLearn how to perform common tasks in MLflow\\n\\n\\nGuides\\n\\nAutologging tutorial for effortless model tracking'),\n",
       "  Document(metadata={'source': 'https://mlflow.org/docs/latest/index.html', 'title': 'MLflow: A Tool for Managing the Machine Learning Lifecycle', 'language': 'en'}, page_content='Contribute\\n\\n\\nDocumentation \\nMLflow: A Tool for Managing the Machine Learning Lifecycle\\n\\n\\nMLflow: A Tool for Managing the Machine Learning Lifecycle \\nMLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in\\nhandling the complexities of the machine learning process. MLflow focuses on the full lifecycle for\\nmachine learning projects, ensuring that each phase is manageable, traceable, and reproducible.'),\n",
       "  Document(metadata={'source': 'https://mlflow.org/docs/latest/python_api/mlflow.deployments.html', 'title': 'mlflow.deployments', 'language': 'en'}, page_content='MLflow Overview\\nGetting Started with MLflow\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nmlflow\\nMLflow Tracing APIs\\nmlflow.artifacts\\nmlflow.autogen\\nmlflow.catboost\\nmlflow.client\\nmlflow.config\\nmlflow.data\\nmlflow.deployments\\nmlflow.diviner\\nmlflow.entities\\nmlflow.environment_variables\\nmlflow.fastai\\nmlflow.gateway\\nmlflow.gluon\\nmlflow.h2o\\nmlflow.johnsnowlabs\\nmlflow.keras\\nmlflow.langchain\\nmlflow.lightgbm\\nmlflow.llama_index\\nmlflow.metrics\\nmlflow.mleap\\nmlflow.models\\nmlflow.onnx\\nmlflow.paddle\\nmlflow.pmdarima\\nmlflow.projects\\nmlflow.promptflow\\nmlflow.prophet\\nmlflow.pyfunc\\nmlflow.pyspark.ml\\nmlflow.pytorch\\nmlflow.recipes\\nmlflow.sagemaker\\nmlflow.sentence_transformers\\nmlflow.server\\nmlflow.shap\\nmlflow.sklearn\\nmlflow.spacy\\nmlflow.spark\\nmlflow.statsmodels\\nmlflow.system_metrics\\nmlflow.tensorflow\\nmlflow.tracing\\nmlflow.transformers\\nmlflow.types\\nmlflow.utils\\nmlflow.xgboost\\nmlflow.openai\\nLog Levels'),\n",
       "  Document(metadata={'source': 'https://mlflow.org/docs/latest/index.html', 'title': 'MLflow: A Tool for Managing the Machine Learning Lifecycle', 'language': 'en'}, page_content='2.17.0\\n\\n\\n MLflow\\n\\nMLflow Overview\\nGetting Started with MLflow\\nNew Features\\nLLMs\\nMLflow Tracing\\nModel Evaluation\\nDeep Learning\\nTraditional ML\\nDeployment\\nMLflow Tracking\\nSystem Metrics\\nMLflow Projects\\nMLflow Models\\nMLflow Model Registry\\nMLflow Recipes\\nMLflow Plugins\\nMLflow Authentication\\nCommand-Line Interface\\nSearch Runs\\nSearch Experiments\\nPython API\\nR API\\nJava API\\nREST API\\nOfficial MLflow Docker Image\\nCommunity Model Flavors\\nTutorials and Examples\\n\\n\\nContribute')],\n",
       " 'question': 'What is MLflow?',\n",
       " 'answer': 'MLflow is an open-source platform designed to manage the machine learning lifecycle. It helps machine learning practitioners and teams handle the complexities of ML projects by making each phase manageable, traceable, and reproducible. MLflow focuses on tracking experiments, packaging code into reproducible runs, and sharing and deploying models.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the chain\n",
    "result = rag_chain_with_source.invoke(\"What is MLflow?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd70bcf6-7c44-44d3-9435-567b82611e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluate the RAG system using MLFlow and RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1064306-b7f3-4b3e-825c-4353d808f21d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create an eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5481491-e4a9-42ea-8a3f-f527faffd04d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>MLflow is an open source platform to manage th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the mlflow.evaluate() function?</td>\n",
       "      <td>The mlflow.evaluate() function evaluates the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I log a table with MLFlow?</td>\n",
       "      <td>You can log a table with MLFlow using the mlfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I load a saved table?</td>\n",
       "      <td>You can load a saved table using the mlflow.lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question  \\\n",
       "0                          What is MLflow?   \n",
       "1  What is the mlflow.evaluate() function?   \n",
       "2       How can I log a table with MLFlow?   \n",
       "3            How can I load a saved table?   \n",
       "\n",
       "                                           reference  \n",
       "0  MLflow is an open source platform to manage th...  \n",
       "1  The mlflow.evaluate() function evaluates the m...  \n",
       "2  You can log a table with MLFlow using the mlfl...  \n",
       "3  You can load a saved table using the mlflow.lo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the evaluation dataset\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is the mlflow.evaluate() function?\",\n",
    "            \"How can I log a table with MLFlow?\",\n",
    "            \"How can I load a saved table?\",\n",
    "        ],\n",
    "        \"reference\": [\n",
    "            \"MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, and deployment.\",\n",
    "            \"The mlflow.evaluate() function evaluates the model on the given dataset.\",\n",
    "            \"You can log a table with MLFlow using the mlflow.log_table() function.\",\n",
    "            \"You can load a saved table using the mlflow.load_table() function.\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the chain on the `eval_df` and extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>[Learn about MLflowMLflow BasicsMLflow Models ...</td>\n",
       "      <td>MLflow is an open-source platform designed to ...</td>\n",
       "      <td>MLflow is an open source platform to manage th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the mlflow.evaluate() function?</td>\n",
       "      <td>[MLflow Overview\\nGetting Started with MLflow\\...</td>\n",
       "      <td>I apologize, but I don't have any specific inf...</td>\n",
       "      <td>The mlflow.evaluate() function evaluates the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I log a table with MLFlow?</td>\n",
       "      <td>[Automatic Logging with MLflow Tracking\\n\\n\\n2...</td>\n",
       "      <td>To log a table with MLflow, you can use the `m...</td>\n",
       "      <td>You can log a table with MLFlow using the mlfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I load a saved table?</td>\n",
       "      <td>[Clicking on the run (“clumsy-steed-426” in th...</td>\n",
       "      <td>I apologize, but I don't have any specific inf...</td>\n",
       "      <td>You can load a saved table using the mlflow.lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_input  \\\n",
       "0                          What is MLflow?   \n",
       "1  What is the mlflow.evaluate() function?   \n",
       "2       How can I log a table with MLFlow?   \n",
       "3            How can I load a saved table?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Learn about MLflowMLflow BasicsMLflow Models ...   \n",
       "1  [MLflow Overview\\nGetting Started with MLflow\\...   \n",
       "2  [Automatic Logging with MLflow Tracking\\n\\n\\n2...   \n",
       "3  [Clicking on the run (“clumsy-steed-426” in th...   \n",
       "\n",
       "                                            response  \\\n",
       "0  MLflow is an open-source platform designed to ...   \n",
       "1  I apologize, but I don't have any specific inf...   \n",
       "2  To log a table with MLflow, you can use the `m...   \n",
       "3  I apologize, but I don't have any specific inf...   \n",
       "\n",
       "                                           reference  \n",
       "0  MLflow is an open source platform to manage th...  \n",
       "1  The mlflow.evaluate() function evaluates the m...  \n",
       "2  You can log a table with MLFlow using the mlfl...  \n",
       "3  You can load a saved table using the mlflow.lo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_metadata = defaultdict(list)\n",
    "samples = []\n",
    "for _, row in eval_df.iterrows():\n",
    "    # Invoke the chain while capturing the token usage/cost\n",
    "    with get_bedrock_anthropic_callback() as cb:\n",
    "        start_time = time.time()\n",
    "        result = rag_chain_with_source.invoke(row[\"question\"])\n",
    "        end_time = time.time()\n",
    "\n",
    "    invoke_metadata[\"user_input\"].append(row[\"question\"])\n",
    "    invoke_metadata[\"total_tokens\"].append(cb.total_tokens)\n",
    "    invoke_metadata[\"total_cost\"].append(cb.total_cost)\n",
    "    invoke_metadata[\"latency\"].append(end_time - start_time)\n",
    "\n",
    "    samples.append(\n",
    "        SingleTurnSample(\n",
    "            user_input=row[\"question\"],\n",
    "            reference=row[\"reference\"],\n",
    "            response=result[\"answer\"],\n",
    "            retrieved_contexts=[i.page_content for i in result[\"context\"]],\n",
    "        )\n",
    "    )\n",
    "\n",
    "metadata_df = pd.DataFrame(invoke_metadata)\n",
    "scoring_dataset = EvaluationDataset(samples=samples)\n",
    "scoring_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>863</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>2.483420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the mlflow.evaluate() function?</td>\n",
       "      <td>938</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>2.757172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I log a table with MLFlow?</td>\n",
       "      <td>641</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>2.670629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I load a saved table?</td>\n",
       "      <td>633</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>2.757582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_input  total_tokens  total_cost   latency\n",
       "0                          What is MLflow?           863    0.003417  2.483420\n",
       "1  What is the mlflow.evaluate() function?           938    0.003486  2.757172\n",
       "2       How can I log a table with MLFlow?           641    0.002799  2.670629\n",
       "3            How can I load a saved table?           633    0.002679  2.757582"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the scoring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:22<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MLflow?</td>\n",
       "      <td>[Learn about MLflowMLflow BasicsMLflow Models ...</td>\n",
       "      <td>MLflow is an open-source platform designed to ...</td>\n",
       "      <td>MLflow is an open source platform to manage th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.962677</td>\n",
       "      <td>0.933165</td>\n",
       "      <td>863</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>2.483420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the mlflow.evaluate() function?</td>\n",
       "      <td>[MLflow Overview\\nGetting Started with MLflow\\...</td>\n",
       "      <td>I apologize, but I don't have any specific inf...</td>\n",
       "      <td>The mlflow.evaluate() function evaluates the m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683917</td>\n",
       "      <td>938</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>2.757172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I log a table with MLFlow?</td>\n",
       "      <td>[Automatic Logging with MLflow Tracking\\n\\n\\n2...</td>\n",
       "      <td>To log a table with MLflow, you can use the `m...</td>\n",
       "      <td>You can log a table with MLFlow using the mlfl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988673</td>\n",
       "      <td>0.895391</td>\n",
       "      <td>641</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>2.670629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I load a saved table?</td>\n",
       "      <td>[Clicking on the run (“clumsy-steed-426” in th...</td>\n",
       "      <td>I apologize, but I don't have any specific inf...</td>\n",
       "      <td>You can load a saved table using the mlflow.lo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868268</td>\n",
       "      <td>633</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>2.757582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_input  \\\n",
       "0                          What is MLflow?   \n",
       "1  What is the mlflow.evaluate() function?   \n",
       "2       How can I log a table with MLFlow?   \n",
       "3            How can I load a saved table?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Learn about MLflowMLflow BasicsMLflow Models ...   \n",
       "1  [MLflow Overview\\nGetting Started with MLflow\\...   \n",
       "2  [Automatic Logging with MLflow Tracking\\n\\n\\n2...   \n",
       "3  [Clicking on the run (“clumsy-steed-426” in th...   \n",
       "\n",
       "                                            response  \\\n",
       "0  MLflow is an open-source platform designed to ...   \n",
       "1  I apologize, but I don't have any specific inf...   \n",
       "2  To log a table with MLflow, you can use the `m...   \n",
       "3  I apologize, but I don't have any specific inf...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  MLflow is an open source platform to manage th...             1.0   \n",
       "1  The mlflow.evaluate() function evaluates the m...             0.0   \n",
       "2  You can log a table with MLFlow using the mlfl...             0.0   \n",
       "3  You can load a saved table using the mlflow.lo...             0.0   \n",
       "\n",
       "   faithfulness  context_precision  answer_relevancy  semantic_similarity  \\\n",
       "0      0.636364                0.5          0.962677             0.933165   \n",
       "1      1.000000                0.0          0.000000             0.683917   \n",
       "2      0.000000                0.0          0.988673             0.895391   \n",
       "3      0.500000                0.0          0.000000             0.868268   \n",
       "\n",
       "   total_tokens  total_cost   latency  \n",
       "0           863    0.003417  2.483420  \n",
       "1           938    0.003486  2.757172  \n",
       "2           641    0.002799  2.670629  \n",
       "3           633    0.002679  2.757582  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the evaluators llms and embeddings\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        temperature=0.4,\n",
    "    )\n",
    ")\n",
    "evaluator_embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client, model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# Define the metrics\n",
    "metrics = [\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness(),\n",
    "    ContextPrecision(),\n",
    "    ResponseRelevancy(),\n",
    "    SemanticSimilarity(),\n",
    "]\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate(\n",
    "    dataset=scoring_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    ")\n",
    "\n",
    "results_df = results.to_pandas()\n",
    "results_df = results_df.merge(metadata_df, on=\"user_input\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea40ce52-6ac7-4c20-9669-d24f80a6cebe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 10:50:39 INFO mlflow.tracking._tracking_service.client: 🏃 View run rambunctious-fly-398 at: http://localhost:8080/#/experiments/511731529628869183/runs/6a98d2ac73134132b3ae6a16fd854929.\n",
      "2024/10/22 10:50:39 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/511731529628869183.\n"
     ]
    }
   ],
   "source": [
    "def create_agg_metrics_dict(df, metrics):\n",
    "    agg_metrics = {}\n",
    "    for metric in metrics:\n",
    "        agg_metrics[metric + \"_mean\"] = df[metric].mean()\n",
    "        agg_metrics[metric + \"_std\"] = df[metric].std()\n",
    "    return agg_metrics\n",
    "\n",
    "\n",
    "# Set out tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Question-Answering Evaluation 0.1\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"text_splitter\": \"langchain.text_splitter.CharacterTextSplitter\",\n",
    "            \"text_splitter__chunk_size\": 500,\n",
    "            \"text_splitter__chunk_overlap\": 100,\n",
    "            \"llm_model\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            \"llm_model__temperature\": 0.1,\n",
    "            \"embedding_model\": \"amazon.titan-embed-text-v1\",\n",
    "            \"user_prompt\": prompt.messages[0].prompt.template,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log the user_prompt\n",
    "    mlflow.log_text(prompt.messages[0].prompt.template, \"user_prompt.txt\")\n",
    "\n",
    "    # Log the evaluation dataset\n",
    "    mlflow.log_input(\n",
    "        mlflow.data.from_pandas(eval_df),\n",
    "        context=\"test\",\n",
    "    )\n",
    "\n",
    "    # Log the evaluation results\n",
    "    mlflow.log_table(\n",
    "        results_df,\n",
    "        \"results_df.json\",\n",
    "    )\n",
    "\n",
    "    # Log the metric results\n",
    "    mlflow.log_metrics(\n",
    "        create_agg_metrics_dict(\n",
    "            results_df,\n",
    "            [\n",
    "                \"context_recall\",\n",
    "                \"faithfulness\",\n",
    "                \"context_precision\",\n",
    "                \"answer_relevancy\",\n",
    "                \"semantic_similarity\",\n",
    "                \"total_tokens\",\n",
    "                \"total_cost\",\n",
    "                \"latency\",\n",
    "            ],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Grid Size:  8\n"
     ]
    }
   ],
   "source": [
    "def product_hyperparameters(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    return [dict(zip(keys, instance)) for instance in product(*kwargs.values())]\n",
    "\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    \"llm_model\": [\n",
    "        \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    ],\n",
    "    \"user_prompt\": [\n",
    "        {\n",
    "            \"v1\": \"Use the following pieces of retrieved context to answer the question.\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        },\n",
    "        {\n",
    "            \"v2\": \"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        },\n",
    "    ],\n",
    "    \"temperature\": [0.1, 0.7],\n",
    "}\n",
    "\n",
    "# Open the hyperparameter json file\n",
    "unique_hyper_product = product_hyperparameters(**hyperparameter_grid)\n",
    "\n",
    "print(\"Hyperparameter Grid Size: \", len(list(unique_hyper_product)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_model': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'user_prompt': {'v1': 'Use the following pieces of retrieved context to answer the question.\\n\\nQuestion: {question}\\nContext: {context}\\nAnswer:'},\n",
       " 'temperature': 0.1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_hyper_product[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_run(params, metrics, evaluator_llm, evaluator_embeddings) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a QA model run using the provided parameters, metrics, and evaluators.\n",
    "    Args:\n",
    "        params (dict): Parameters for building the QA chain.\n",
    "        metrics (list): List of metrics to evaluate the model.\n",
    "        evaluator_llm (object): The language model evaluator.\n",
    "        evaluator_embeddings (object): The embeddings evaluator.\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"eval_df\" (pd.DataFrame): The evaluation DataFrame.\n",
    "            - \"results_df\" (pd.DataFrame): The results DataFrame with evaluation metrics and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build qa chain\n",
    "    qa_chain = build_qa_chain(params)\n",
    "\n",
    "    # Build scoring and metadata dataset\n",
    "    print(\"Running the QA chain on the evaluation dataset...\")\n",
    "    invoke_metadata = defaultdict(list)\n",
    "    samples = []\n",
    "    for _, row in eval_df.iterrows():\n",
    "        # Invoke the chain while capturing the token usage/cost\n",
    "        with get_bedrock_anthropic_callback() as cb:\n",
    "            start_time = time.time()\n",
    "            result = qa_chain.invoke(row[\"question\"])\n",
    "            end_time = time.time()\n",
    "\n",
    "        invoke_metadata[\"user_input\"].append(row[\"question\"])\n",
    "        invoke_metadata[\"total_tokens\"].append(cb.total_tokens)\n",
    "        invoke_metadata[\"total_cost\"].append(cb.total_cost)\n",
    "        invoke_metadata[\"latency\"].append(end_time - start_time)\n",
    "\n",
    "        samples.append(\n",
    "            SingleTurnSample(\n",
    "                user_input=row[\"question\"],\n",
    "                reference=row[\"reference\"],\n",
    "                response=result[\"answer\"],\n",
    "                retrieved_contexts=[i.page_content for i in result[\"context\"]],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    metadata_df = pd.DataFrame(invoke_metadata)\n",
    "    scoring_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the QA chain...\")\n",
    "    results = evaluate(\n",
    "        dataset=scoring_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=evaluator_embeddings,\n",
    "    )\n",
    "\n",
    "    results_df = results.to_pandas()\n",
    "    results_df = results_df.merge(metadata_df, on=\"user_input\")\n",
    "    return {\"eval_df\": eval_df, \"results_df\": results_df}\n",
    "\n",
    "\n",
    "def log_run(run_output: dict) -> None:\n",
    "    \"\"\"\n",
    "    Logs the run output to MLflow as a nested run.\n",
    "\n",
    "    Parameters:\n",
    "    - run_output (dict): A dictionary containing the run output data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"Logging the run output to MLflow...\")\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(run_output[\"params\"])\n",
    "\n",
    "    # Log the user_prompt\n",
    "    mlflow.log_text(run_output[\"user_prompt\"], \"user_prompt.txt\")\n",
    "\n",
    "    # Log the evaluation dataset\n",
    "    mlflow.log_input(\n",
    "        mlflow.data.from_pandas(run_output[\"eval_df\"]),\n",
    "        context=\"test\",\n",
    "    )\n",
    "\n",
    "    # Log the evaluation results\n",
    "    mlflow.log_table(\n",
    "        run_output[\"results_df\"],\n",
    "        \"results_df.json\",\n",
    "    )\n",
    "\n",
    "    # Log the metric results\n",
    "    mlflow.log_metrics(\n",
    "        create_agg_metrics_dict(\n",
    "            run_output[\"results_df\"],\n",
    "            [\n",
    "                \"context_recall\",\n",
    "                \"faithfulness\",\n",
    "                \"context_precision\",\n",
    "                \"answer_relevancy\",\n",
    "                \"semantic_similarity\",\n",
    "                \"total_tokens\",\n",
    "                \"total_cost\",\n",
    "                \"latency\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def build_qa_chain(params):\n",
    "    # Build qa chain\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    llm = ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model=params[\"llm_model\"],\n",
    "        temperature=params[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | ChatPromptTemplate.from_template(list(params[\"user_prompt\"].values())[0])\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "    return rag_chain_with_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating on each unique Hyperparameter combination:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the QA chain on the evaluation dataset...\n",
      "Evaluating the QA chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[1]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 20/20 [03:00<00:00,  9.00s/it]\n",
      "2024/10/22 10:56:47 INFO mlflow.tracking._tracking_service.client: 🏃 View run whimsical-robin-126 at: http://localhost:8080/#/experiments/183325462269967356/runs/c03f781176ea42969492139d777fa200.\n",
      "2024/10/22 10:56:47 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/183325462269967356.\n",
      "Iterating on each unique Hyperparameter combination:  12%|█▎        | 1/8 [03:37<25:24, 217.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the run output to MLflow...\n",
      "Running the QA chain on the evaluation dataset...\n",
      "Evaluating the QA chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Prompt response_relevance_prompt failed to parse output: The output parser failed to parse the output after 0 retries.\n",
      "Exception raised in Job[13]: RagasOutputParserException(The output parser failed to parse the output after 0 retries.)\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 20/20 [03:00<00:00,  9.00s/it]\n",
      "2024/10/22 11:00:20 INFO mlflow.tracking._tracking_service.client: 🏃 View run charming-elk-103 at: http://localhost:8080/#/experiments/183325462269967356/runs/21b3a2ea0ec74433b5269d798d3e1d86.\n",
      "2024/10/22 11:00:20 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/183325462269967356.\n",
      "Iterating on each unique Hyperparameter combination:  25%|██▌       | 2/8 [07:10<21:29, 214.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the run output to MLflow...\n",
      "Running the QA chain on the evaluation dataset...\n",
      "Evaluating the QA chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:21<00:00,  1.07s/it]\n",
      "2024/10/22 11:00:53 INFO mlflow.tracking._tracking_service.client: 🏃 View run salty-grub-820 at: http://localhost:8080/#/experiments/183325462269967356/runs/939e3b0ca0f54a618755ebc1f22865e2.\n",
      "2024/10/22 11:00:53 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/183325462269967356.\n",
      "Iterating on each unique Hyperparameter combination:  38%|███▊      | 3/8 [07:44<11:01, 132.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the run output to MLflow...\n",
      "Running the QA chain on the evaluation dataset...\n",
      "Evaluating the QA chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:23<00:00,  1.17s/it]\n",
      "2024/10/22 11:01:31 INFO mlflow.tracking._tracking_service.client: 🏃 View run zealous-foal-236 at: http://localhost:8080/#/experiments/183325462269967356/runs/bf3c9f3a7ba140c0a1ec9b2f18a6fca6.\n",
      "2024/10/22 11:01:31 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/183325462269967356.\n",
      "Iterating on each unique Hyperparameter combination:  50%|█████     | 4/8 [08:21<06:18, 94.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the run output to MLflow...\n",
      "Running the QA chain on the evaluation dataset...\n",
      "Evaluating the QA chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define the evaluators llms and embeddings\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        temperature=0.4,\n",
    "    )\n",
    ")\n",
    "evaluator_embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_client, model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# Define the metrics\n",
    "metrics = [\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness(),\n",
    "    ContextPrecision(),\n",
    "    ResponseRelevancy(),\n",
    "    SemanticSimilarity(),\n",
    "]\n",
    "\n",
    "# Set out tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Question-Answering Evaluation 0.2\")\n",
    "\n",
    "# Enable LangChain autologging\n",
    "# Note that models and examples are not required to be logged in order to log traces.\n",
    "# Simply enabling autolog for LangChain via mlflow.langchain.autolog() will enable trace logging.\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "experiment_cost = 0\n",
    "# Iterate through combinations of hyperparameters\n",
    "for params in tqdm(\n",
    "    unique_hyper_product,\n",
    "    desc=\"Iterating on each unique Hyperparameter combination\",\n",
    "):\n",
    "    # Create the run output dictionary\n",
    "    params_with_tags = {\n",
    "        k: v if not isinstance(v, dict) else list(v.keys())[0]\n",
    "        for k, v in params.items()\n",
    "    }\n",
    "    output_map = {\n",
    "        \"params\": params_with_tags,\n",
    "        \"user_prompt\": list(params[\"user_prompt\"].values())[0],\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Evaluate the run\n",
    "        run_output = evaluate_run(params, metrics, evaluator_llm, evaluator_embeddings)\n",
    "\n",
    "        # Add the cost of the experiment to the total cost\n",
    "        experiment_cost += run_output[\"results_df\"][\"total_cost\"].sum()\n",
    "\n",
    "        # Log mlflow run\n",
    "        log_run(output_map | run_output)\n",
    "\n",
    "\n",
    "print(f\"Total Cost of Experiment: {experiment_cost} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "\n",
    "https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing\n",
    "\n",
    "https://github.com/LokaHQ/mlops-tech-stack/tree/main/monitoring/mlflow-tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Server\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking/server.html\n",
    "\n",
    "https://github.com/LokaHQ/mlops-tech-stack/tree/main/experiment-tracking/sagemaker-mlflow-managed\n",
    "\n",
    "https://github.com/LokaHQ/mlops-tech-stack/tree/main/experiment-tracking/mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering UI\n",
    "\n",
    "https://mlflow.org/docs/latest/llms/prompt-engineering/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM Evaluation Examples -- RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "mlflow-monitoring-pFwyWwNg-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
